{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This is the documentation website for the LGSVL Simulator project. You can subscribe to our email newsletter here . Visit our website here: https://www.lgsvlsimulator.com Visit our Github here: https://github.com/lgsvl/simulator Quick Start Getting Started Building from source Simulator controls Ground truth obstacles Integration with AD Running with Autoware Running with Apollo 3.5 Running with Apollo 3.0 Python API Python API Examples Tutorials Map Annotation NPC Map Navigation How to add a new ego vehicle How to create a simple ROS2-based AD stack with LGSVL Simulator Support Frequently Asked Questions Contributing","title":"Home"},{"location":"#home","text":"This is the documentation website for the LGSVL Simulator project. You can subscribe to our email newsletter here . Visit our website here: https://www.lgsvlsimulator.com Visit our Github here: https://github.com/lgsvl/simulator","title":"Home"},{"location":"#quick-start","text":"Getting Started Building from source Simulator controls Ground truth obstacles","title":"Quick Start"},{"location":"#integration-with-ad","text":"Running with Autoware Running with Apollo 3.5 Running with Apollo 3.0 Python API Python API Examples","title":"Integration with AD"},{"location":"#tutorials","text":"Map Annotation NPC Map Navigation How to add a new ego vehicle How to create a simple ROS2-based AD stack with LGSVL Simulator","title":"Tutorials"},{"location":"#support","text":"Frequently Asked Questions Contributing","title":"Support"},{"location":"add-new-ego-vehicle/","text":"How to Add a New Ego Vehicle This document will describe how to create a new ego vehicle in the LGSVL Simulator. Video ( Link ) Getting Started The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive . Create and Name Your Vehicle Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later. Add Child Components Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab. Apply the Correct References We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references. Vehicle Controller script For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center Car HeadLights script For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot Car Input Controller script For the Car Input Controller script, reference the DriverCamera. Force Feedback script For the Force Feedback script, reference the FL and FR WheelColliders. Vehicle Animation Manager script For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator) Vehicle Position Resetter script For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray. Agent Setup script For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes Final Steps Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle!","title":"How to add a new ego vehicle"},{"location":"add-new-ego-vehicle/#how-to-add-a-new-ego-vehicle","text":"This document will describe how to create a new ego vehicle in the LGSVL Simulator.","title":"How to Add a New Ego Vehicle"},{"location":"add-new-ego-vehicle/#video","text":"( Link )","title":"Video"},{"location":"add-new-ego-vehicle/#getting-started","text":"The following text is a list of the steps described in the above YouTube video. Launch LGSVL Simulator from the Unity Editor (as described here ). Create a new scene and add an existing ego vehicle prefab to the hierarchy. This makes it easy to copy components to the new vehicle. Toggle the reference vehicle prefab to inactive .","title":"Getting Started"},{"location":"add-new-ego-vehicle/#create-and-name-your-vehicle","text":"Create a new empty root gameobject for your vehicle and give it a name. Place vehicle meshes as a child of the root gameobject. Assign root gameobject tag to \"Player\". Right click on each component on the reference prefab root and copy it. Then paste each copied component onto the new vehicle gameobject root. You'll need to go back to the reference prefab root to copy each additional object before pasting. Note: We will fix the missing references later.","title":"Create and Name Your Vehicle"},{"location":"add-new-ego-vehicle/#add-child-components","text":"Add the following components as children: MainCollider WheelColliders Lights GroundTruthDetectBoundingBox DriverCamera DriverCameraPositions DashInteriorUICanvas SensorArray Note: These are prefabs that were created from an existing ego vehicle prefab. For this tutorial the hierarchy and scripts were adjusted. You don't need to alter them to match; just use the reference vehicle. Next, drag the root object into the project panel to create a prefab. This will serialize the gameobject as a prefab.","title":"Add Child Components"},{"location":"add-new-ego-vehicle/#apply-the-correct-references","text":"We need to apply the correct references to the vehicle scripts since the public variables are still referencing the other vehicle: Note: Be sure to click the Apply button in the Inspector after each component change. This saves the change to the prefab in the project. Check that no variable references are still bold after updating the references.","title":"Apply the Correct References"},{"location":"add-new-ego-vehicle/#vehicle-controller-script","text":"For the Vehicle Controller script, reference the following colliders and meshes: Reference the FL WheelColliders in Axles Element 0 (Left) Reference the FR WheelColliders in Axles Element 0 (Right) Reference the RL WheelColliders in Axles Element 1 (Left) Reference the RR WheelColliders in Axles Element 1 (Right) Reference the FL_PARENT WheelMeshes in Axles Element 0 (Left Visuals) Reference the FR_PARENT WheelMeshes in Axles Element 0 (Right Visuals) Reference the RL_PARENT WheelMeshes in Axles Element 1 (Left Visuals) Reference the RR_PARENT WheelMeshes in Axles Element 1 (Right Visuals) Reference the MainCollider in Car Center","title":"Vehicle Controller script"},{"location":"add-new-ego-vehicle/#car-headlights-script","text":"For the Car Headlights script, reference the following Lights: Reference the XE Left Headlight Spot Reference the XE Right Headlight Spot Reference the XE Left Tail Spot Reference the XE Right Tail Spot","title":"Car HeadLights script"},{"location":"add-new-ego-vehicle/#car-input-controller-script","text":"For the Car Input Controller script, reference the DriverCamera.","title":"Car Input Controller script"},{"location":"add-new-ego-vehicle/#force-feedback-script","text":"For the Force Feedback script, reference the FL and FR WheelColliders.","title":"Force Feedback script"},{"location":"add-new-ego-vehicle/#vehicle-animation-manager-script","text":"For the Vehicle Animation Manager script, reference the following meshes: Reference WiperLeft mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperLeft (Animator) Reference WiperRight mesh object (under MeshHolder, DashMeshes, then WindshieldWipers) in WiperRight (Animator)","title":"Vehicle Animation Manager script"},{"location":"add-new-ego-vehicle/#vehicle-position-resetter-script","text":"For the Vehicle Position Resetter script, reference the GpsSensor under SensorArray.","title":"Vehicle Position Resetter script"},{"location":"add-new-ego-vehicle/#agent-setup-script","text":"For the Agent Setup script, update the following references: Reference the DriverCamera in Follow Camera. Reference the NewVehicle in Camera Man. Note that the AgentSetup script has an extra step needed to reference bridge classes in each object of the Needs Bridge array. To do this, you'll need to drag each class from a second inspector panel: Add a new Inspector tab next to the Console tab. Lock one panel and use the other to select the sensor object. Then drag the class into the NeedsBridge array. Do this for all sensors that require a bridge connection: LidarSensor (from sensor inspector) GpsSensor (from sensor inspector) TelephotoCamera (from sensor inspector) CaptureCamera (from sensor inspector) ImuSensor (from sensor inspector) RadarSensor (from sensor inspector) VehicleInputController (from NewVehicle inspector) CanBusSensor (from sensor inspector) SegmentationCamera (from sensor inspector) VehiclePositionResetter (from NewVehicle inspector) UserInterfaceTweakables (from NewVehicle inspector) Unlock and close the extra inspector panel Now select NewVehicle, and click the Apply button to apply changes. Next, update the child objects public references: For the Driver Camera, update the following camera position items: DriverCameraPosition ThirdPersonCameraPosition ReverseViewCameraPosition For Cam Fix To, update Fix To with ThirdPersonCameraPosition For Cam Smooth Follow: Update Target Position Transform with ThirdPersonCameraPosition Update Target Object with NewVehicle Click the Apply button to apply changes Next, update the SensorArray public references: For Can Bus script: Update MainRigidBody with NewVehicle Update Controller with NewVehicle Update Input_controller with NewVehicle Update Gps with GpsSensor Click the Apply button to apply changes For GpsSensor script: Update Target with NewVehicle Update Agent with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes For ImuSensor script: Update Target with NewVehicle Update MainRigidBody with NewVehicle Click the Apply button to apply changes","title":"Agent Setup script"},{"location":"add-new-ego-vehicle/#final-steps","text":"Set the vehicle and all child objects to the Duckiebot layer. Next, apply changes, delete the reference ego vehicle, and save the scene. Finally, select the ROSAgentManager prefab from the project and increase the size of the AgentPrefabs array by one. Add the NewVehicle prefab to the Agent Prefabs array. Be sure to add the prefab from the PROJECT panel, not the scene! Press Play to launch the new scene. Click the Vehicle popup to see the new vehicle in the vehicle list. Congratulations! You have successfully added a new ego vehicle!","title":"Final Steps"},{"location":"api-quickstart-descriptions/","text":"Python API Quickstart Script Descriptions This document describes the example Python scripts that use the LGSVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LIDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the npc is printed 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API Examples"},{"location":"api-quickstart-descriptions/#python-api-quickstart-script-descriptions","text":"This document describes the example Python scripts that use the LGSVL Simulator Python API. These scripts are located here . You can find the documentation on the API here . 01-connecting-to-simulator.py : How to connect to an already running instance of the simulator and some information you can get about the instance 02-loading-scene-show-spawns.py : How to load a scene and get the scene's predefined spawn transforms 03-raycast.py : How to create an EGO vehicle and do raycasting from a point 04-ego-drive-straight.py : How to create an agent with a velocity and then run the simulator for a set amount of time 05-ego-drive-in-circle.py : How to apply control to an EGO vehicle and then run the simulator indefinitely 06-save-camera-image.py : How to save a camera image in different formats and with various settings 07-save-lidar-point-cloud.py : How to save a LIDAR point cloud 08-create-npc.py : How to create several types of NPC vehicles and spawn them in different positions 09-reset-scene.py : How to empty the scene of all EGOs, NPCs, and Pedestrians, but keep the scene loaded 10-npc-follow-the-lane.py : How to create NPCs and then let them drive in the nearest annotated lane 11-collision-callbacks.py : How to setup the simulator so that whenever the 3 created agents collide with anything, the name of the agent and the collision point is printed 12-create-npc-on-lane.py : How to create NPC vehicles in random position in a radius around the EGO vehicle, but the NPCs are placed on the nearest lane to the initial random position 13-npc-follow-waypoints.py : How to create a list of waypoints and direct an NPC to follow them 14-create-pedestrians.py : How to create pedestrians in rows in front of the spawn position 15-pedestrian-walk-randomly.py : How to start and stop a pedestrian walking randomly on the sidewalk 16-pedestrian-follow-waypoints.py : How to create a list of waypoints and direct a pedestrian to follow them 17-many-pedestrians-walking.py : How to generate an army of pedestrians and have them walk back and forth 18-weather-effects.py : How to get the current weather state of the simulator and how to adjust the various settings 19-time-of-day.py : How to get the time of date in the simulator and how to set it to a fixed time and a moving time 20-enable-sensors.py : How to enable a specific sensor so that it can send data over a bridge 21-map-coordinates.py : How to convert from simulator coordinates to GPS coordinates and back. Latitude/Longitude and Northing/Easting are supported along with altitude and orientation 22-connecting-bridge.py : How to command an EGO vehicle to connect to a bridge at a specific IP address and port and then wait for the connection to be established 23-npc-callbacks.py : How to setup the simulator so that whenever an NPC reaches a stopline or changes lane, the name of the npc is printed 99-utils-examples.py : How to use several of the utility scripts to transform an arbitrary point to the coordinate system of a local transform (relative to sensor)","title":"Python API Quickstart Script Descriptions"},{"location":"apollo-instructions/","text":"Running Apollo 3.0 with LGSVL Simulator Table of Contents Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and Rosbridge Launching Apollo Alongside the Simulator Getting Started The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Prerequisites Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup Docker Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing Nvidia Docker Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo Cloning the Repository This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git Building Apollo and rosbridge Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make Launching Apollo alongside the simulator Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: ./scripts/bootstrap.sh Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Select the San Francisco map and the XE-Rigged-apollo vehicle. Enable GPS, IMU, LIDAR, HD Mode, and Telephota Camera. (optional) Enable traffic. Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the LG SVL 1080 vehicle and San Francisco map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container).","title":"Running with Apollo 3.0"},{"location":"apollo-instructions/#running-apollo-30-with-lgsvl-simulator","text":"","title":"Running Apollo 3.0 with LGSVL Simulator"},{"location":"apollo-instructions/#table-of-contents","text":"Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and Rosbridge Launching Apollo Alongside the Simulator","title":"Table of Contents"},{"location":"apollo-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . We use our forked version of the Apollo repository, which can be found here . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle.","title":"Getting Started"},{"location":"apollo-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo-instructions/#setup","text":"","title":"Setup"},{"location":"apollo-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"apollo-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"apollo-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo","title":"Pulling LGSVL Docker image"},{"location":"apollo-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and rosbrige. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Cloning the Repository"},{"location":"apollo-instructions/#building-apollo-and-rosbridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu (optional) to build without gpu: ./apollo.sh build Now build rosbrige: cd ros_pkgs catkin_make","title":"Building Apollo and rosbridge"},{"location":"apollo-instructions/#launching-apollo-alongside-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: ./scripts/bootstrap.sh Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. Launch rosbridge: ./scripts/rosbridge.sh Run the LG SVL Simulator (see instructions in the simulator repository ) Select the San Francisco map and the XE-Rigged-apollo vehicle. Enable GPS, IMU, LIDAR, HD Mode, and Telephota Camera. (optional) Enable traffic. Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the LG SVL 1080 vehicle and San Francisco map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Perception , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_stop.sh script in apollo/docker/scripts in a new terminal (not in the docker container).","title":"Launching Apollo alongside the simulator"},{"location":"apollo3-5-instructions/","text":"Running Apollo 3.5 with LGSVL Simulator This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle. Table of Contents Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and Bridge Launching Apollo Alongside the Simulator Getting Started The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here . Prerequisites Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs). Setup Docker Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made. Installing Docker CE To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing Nvidia Docker Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling LGSVL Docker image LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo-3.5 Cloning the Repository This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-3.5.git Building Apollo and bridge Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu Launching Apollo alongside the simulator Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: bootstrap.sh Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Select the San Francisco map and the XE-Rigged-apollo_3_5 vehicle. Enable GPS, IMU, LIDAR, Main Camera, and Telephoto Camera. (optional) Enable Sensor Effects, Traffic and Pedestrian. Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the XE_Rigged_Apollo3.5 vehicle and San Francisco map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container).","title":"Running with Apollo 3.5"},{"location":"apollo3-5-instructions/#running-apollo-35-with-lgsvl-simulator","text":"This repository is a fork of Apollo maintained by the LG Electronics Silicon Valley Lab which has modified and configured to facilitate use with LG's Automotive Simulator . The software and source code in this repository are intended only for use with LG Automotive Simulator and should not be used in a real vehicle.","title":"Running Apollo 3.5 with LGSVL Simulator"},{"location":"apollo3-5-instructions/#table-of-contents","text":"Getting Started Prerequisites Setup Docker Cloning the Repository Building Apollo and Bridge Launching Apollo Alongside the Simulator","title":"Table of Contents"},{"location":"apollo3-5-instructions/#getting-started","text":"The guide outlines the steps required to setup Apollo for use with the LG Automotive Simulator. If you have not already set up the simulator, please do so first by following the instructions here .","title":"Getting Started"},{"location":"apollo3-5-instructions/#prerequisites","text":"Linux operating system (preferably Ubuntu 14.04 or later) Nvidia graphics card (required for Perception) Nvidia proprietary driver must be installed The current version of Apollo does not support Volta and Turing architectures (this includes Titan V and RTX 2080 GPUs).","title":"Prerequisites"},{"location":"apollo3-5-instructions/#setup","text":"","title":"Setup"},{"location":"apollo3-5-instructions/#docker","text":"Apollo is designed to run out of docker containers. The image will mount this repository as a volume so the image will not need to be rebuilt each time a modification is made.","title":"Docker"},{"location":"apollo3-5-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"apollo3-5-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"apollo3-5-instructions/#pulling-lgsvl-docker-image","text":"LGSVL maintains a docker image to be used alongside this repository. The docker image is available here . To pull the image use the following command: docker pull lgsvl/apollo-3.5","title":"Pulling LGSVL Docker image"},{"location":"apollo3-5-instructions/#cloning-the-repository","text":"This repository includes a couple of submodules for HD Maps and lgsvl msgs. To make sure that the submodules are also cloned use the following command: git clone --recurse-submodules https://github.com/lgsvl/apollo-3.5.git","title":"Cloning the Repository"},{"location":"apollo3-5-instructions/#building-apollo-and-bridge","text":"Now everything should be in place to build apollo. Apollo must be built from the container. To launch the container navigate to the directory where the repository was cloned and enter: ./docker/scripts/dev_start.sh This should launch the container and mount a few volumes. It could take a few minutes to pull the latest volumes on the first run. To get into the container: ./docker/scripts/dev_into.sh Build Apollo: ./apollo.sh build_gpu","title":"Building Apollo and bridge"},{"location":"apollo3-5-instructions/#launching-apollo-alongside-the-simulator","text":"Here we only describe only a simple case of driving from point A to point B using Apollo and the simulator. To launch apollo, first launch and enter a container as described in the previous steps. To start Apollo: bootstrap.sh Note: you may receive errors about dreamview not being build if you do not run the script from the /apollo directory. Launch bridge (inside docker container): bridge.sh Run the LG SVL Simulator outside of docker. See instructions in the simulator repository Select the San Francisco map and the XE-Rigged-apollo_3_5 vehicle. Enable GPS, IMU, LIDAR, Main Camera, and Telephoto Camera. (optional) Enable Sensor Effects, Traffic and Pedestrian. Open Apollo dreamview in a browser by navigating to: localhost:8888 Select the XE_Rigged_Apollo3.5 vehicle and San Francisco map in the top right corner. Open the Module Controller tap (on the left bar). Enable Localization , Transform , Perception , Traffic Light , Planning , Prediction , Routing , and Control . Navigate to the Route Editing tab. Select a destination by clicking on a lane line and clicking Submit Route . Watch the vehicle navigate to the destination. To stop the docker container run the dev_start.sh stop script in apollo/docker/scripts in a new terminal (not in the docker container).","title":"Launching Apollo alongside the simulator"},{"location":"autoware-instructions/","text":"Autoware with LGSVL Simulator Table of Contents General Setup Requirements Docker image setup Simulator Installation Launching Autoware alongside Simulator Copyright and License General This guide goes through how to run Autoware with the LG SVL Simulator. This link leads to our fork of the ROS-based open-source software Autoware. The repository contains various fixes and changes on top of Autoware to allow running it with LG Silicon Valley Lab's Autonomous Driving Simulator. This fork of Autoware is currently rebased on Autoware release 1.9.0. In order to run Autoware with the LGSVL simulator, it is easiest to build and run a custom Docker image. It will also be necessary to clone LGSVL's autoware-data repository, which contains the HD maps, point cloud maps, and launch scripts needed to run Autoware in the simulator's default San Francisco environment. Autoware communicates with the simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers. Setup Requirements Linux operating system Nvidia graphics card Docker image setup We recommend using a Docker container to run Autoware. We do not currently provide an image on Docker Hub, so it is necessary to build the Docker image first manually. Installing Docker CE To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing Nvidia Docker Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Building LGSVL Autoware Docker image Cloning requires instaling Git LFS. If you do not already have Git LFS installed, follow these instructions . Clone this repository to your home directory, making sure to checkout the lgsvl_develop branch: $ git clone --recurse-submodules https://github.com/lgsvl/Autoware.git -b lgsvl_develop Build the Docker image. This should take some time. $ cd autoware/docker/generic $ ./build.sh kinetic You should now have a Docker image named autoware-kinetic . We also need LGSVL's autoware-data repository, which contains map files and launch scripts for running with the simulator. This repository also uses Git LFS. Clone the autoware-data repository: $ mkdir ~/shared_dir cd ~/shared_dir $ git clone https://github.com/lgsvl/autoware-data.git You are now ready to bring up a Docker container and run Autoware. Launch the container: $ cd ~/autoware/docker/generic $ ./run.sh kinetic You should now be logged into the container under the username autoware . Simulator installation Follow the instructions on our simulator Github page here . Launching Autoware alongside LGSVL Simulator To launch Autoware, first bring up the Docker container as described in the previous section. Inside the container, run Autoware: autoware@[MY_DESKTOP]:~$ cd ~/autoware/ros autoware@[MY_DESKTOP]:~$ ./run A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/my_launch_sf_map/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which brings up rosbridge. Run the LG SVL simulator, choosing \"SanFrancisco\" map and \"XE_Rigged-autoware\" for Robot. After \"my_sensing_simulator.launch\" has been loaded, the simulator should show \"Connected\", showing that the simulator has established a connection with the rosbridge server. Click \"Run\" to start, and a vehicle should appear in the streets of San Francisco. On the left, click the check boxes for \"Enable GPS\" and \"Enable LIDAR\". In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Detection,\" \"Mission Planning\", then \"Motion Planning\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. To see the vehicle drive, click \"2D Nav Goal\" in Rviz, then click a destination point and drag slightly in a feasible direction (in the direction of the driving lane) to give a goal destination pose to the planner. The vehicle should plan a path and begin driving towards the destination. You should see something similar in RViz to the above when the vehicle successfully starts navigating using Autoware. Copyright and License This software contains code licensed as described in LICENSE.","title":"Running with Autoware"},{"location":"autoware-instructions/#autoware-with-lgsvl-simulator","text":"","title":"Autoware with LGSVL Simulator"},{"location":"autoware-instructions/#table-of-contents","text":"General Setup Requirements Docker image setup Simulator Installation Launching Autoware alongside Simulator Copyright and License","title":"Table of Contents"},{"location":"autoware-instructions/#general","text":"This guide goes through how to run Autoware with the LG SVL Simulator. This link leads to our fork of the ROS-based open-source software Autoware. The repository contains various fixes and changes on top of Autoware to allow running it with LG Silicon Valley Lab's Autonomous Driving Simulator. This fork of Autoware is currently rebased on Autoware release 1.9.0. In order to run Autoware with the LGSVL simulator, it is easiest to build and run a custom Docker image. It will also be necessary to clone LGSVL's autoware-data repository, which contains the HD maps, point cloud maps, and launch scripts needed to run Autoware in the simulator's default San Francisco environment. Autoware communicates with the simulator using the rosbridge_suite, which provides JSON interfacing with ROS publishers/subscribers.","title":"General"},{"location":"autoware-instructions/#setup","text":"","title":"Setup"},{"location":"autoware-instructions/#requirements","text":"Linux operating system Nvidia graphics card","title":"Requirements"},{"location":"autoware-instructions/#docker-image-setup","text":"We recommend using a Docker container to run Autoware. We do not currently provide an image on Docker Hub, so it is necessary to build the Docker image first manually.","title":"Docker image setup"},{"location":"autoware-instructions/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"autoware-instructions/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate Nvidia driver installed. To test if nvidia drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing Nvidia Docker"},{"location":"autoware-instructions/#building-lgsvl-autoware-docker-image","text":"Cloning requires instaling Git LFS. If you do not already have Git LFS installed, follow these instructions . Clone this repository to your home directory, making sure to checkout the lgsvl_develop branch: $ git clone --recurse-submodules https://github.com/lgsvl/Autoware.git -b lgsvl_develop Build the Docker image. This should take some time. $ cd autoware/docker/generic $ ./build.sh kinetic You should now have a Docker image named autoware-kinetic . We also need LGSVL's autoware-data repository, which contains map files and launch scripts for running with the simulator. This repository also uses Git LFS. Clone the autoware-data repository: $ mkdir ~/shared_dir cd ~/shared_dir $ git clone https://github.com/lgsvl/autoware-data.git You are now ready to bring up a Docker container and run Autoware. Launch the container: $ cd ~/autoware/docker/generic $ ./run.sh kinetic You should now be logged into the container under the username autoware .","title":"Building LGSVL Autoware Docker image"},{"location":"autoware-instructions/#simulator-installation","text":"Follow the instructions on our simulator Github page here .","title":"Simulator installation"},{"location":"autoware-instructions/#launching-autoware-alongside-lgsvl-simulator","text":"To launch Autoware, first bring up the Docker container as described in the previous section. Inside the container, run Autoware: autoware@[MY_DESKTOP]:~$ cd ~/autoware/ros autoware@[MY_DESKTOP]:~$ ./run A few terminals will open, as well as a GUI for the runtime manager. In the runtime manager, click on the 'Quick Start' tab and load the following launch files from ~/shared_dir/autoware-data/my_launch_sf_map/ by clicking \"Ref\" to the right of each text box: my_map.launch my_sensing_simulator.launch my_localization.launch my_detection.launch my_mission_planning.launch my_motion_planning.launch Click \"Map\" to load the launch file pertaining to the HD maps. An \"Ok\" should appear to the right of the \"Ref\" button when successfully loaded. Then click \"Sensing\" which brings up rosbridge. Run the LG SVL simulator, choosing \"SanFrancisco\" map and \"XE_Rigged-autoware\" for Robot. After \"my_sensing_simulator.launch\" has been loaded, the simulator should show \"Connected\", showing that the simulator has established a connection with the rosbridge server. Click \"Run\" to start, and a vehicle should appear in the streets of San Francisco. On the left, click the check boxes for \"Enable GPS\" and \"Enable LIDAR\". In the Autoware Runtime Manager, continue loading the other launch files - click \"Localization\" and wait for the time to display to the right of \"Ref\". Then click \"Detection,\" \"Mission Planning\", then \"Motion Planning\". Then click \"Rviz\" to launch Rviz - the vector map and location of the vehicle in the map should show. To see the vehicle drive, click \"2D Nav Goal\" in Rviz, then click a destination point and drag slightly in a feasible direction (in the direction of the driving lane) to give a goal destination pose to the planner. The vehicle should plan a path and begin driving towards the destination. You should see something similar in RViz to the above when the vehicle successfully starts navigating using Autoware.","title":"Launching Autoware alongside LGSVL Simulator"},{"location":"autoware-instructions/#copyright-and-license","text":"This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"build-instructions/","text":"Instructions to build standalone executable Build steps for Ubuntu host: Install Unity 2018.2.4f1: https://beta.unity3d.com/download/fe703c5165de/public_download.html install into the /opt/Unity folder: chmod +x ~/Downloads/UnitySetup-2018.2.4f1 ./UnitySetup-2018.2.4f1 --unattended --install-location=/opt/Unity --components=Unity,Windows-Mono run Unity and make sure it's activated Make sure you have git-lfs installed before cloning this repository . Clone simulator from GitHub: git clone https://github.com/lgsvl/simulator.git Run build: mkdir build /opt/Unity/Editor/Unity \\ -batchmode \\ -nographics \\ -silent-crashes \\ -quit \\ -buildDestination ./build/simulator \\ -buildTarget Linux64 \\ -executeMethod BuildScript.Build \\ -projectPath . \\ -logFile /dev/stdout Test simulator: Run rosbridge: roslaunch rosbridge_server rosbridge_websocket.launch Run simulator from build/simulator Choose \"Free Roaming\" - \"DuckieDowntown\" as map - \"Duckiebot-duckietown-ros1\" as robot make sure it's connected, click \"RUN\" - make sure it's running, you can operate the robot Run rviz or rqt_image_view and see image from topic \"/simulator/camera_node/image/compressed\"","title":"Build instructions"},{"location":"build-instructions/#instructions-to-build-standalone-executable","text":"Build steps for Ubuntu host: Install Unity 2018.2.4f1: https://beta.unity3d.com/download/fe703c5165de/public_download.html install into the /opt/Unity folder: chmod +x ~/Downloads/UnitySetup-2018.2.4f1 ./UnitySetup-2018.2.4f1 --unattended --install-location=/opt/Unity --components=Unity,Windows-Mono run Unity and make sure it's activated Make sure you have git-lfs installed before cloning this repository . Clone simulator from GitHub: git clone https://github.com/lgsvl/simulator.git Run build: mkdir build /opt/Unity/Editor/Unity \\ -batchmode \\ -nographics \\ -silent-crashes \\ -quit \\ -buildDestination ./build/simulator \\ -buildTarget Linux64 \\ -executeMethod BuildScript.Build \\ -projectPath . \\ -logFile /dev/stdout Test simulator: Run rosbridge: roslaunch rosbridge_server rosbridge_websocket.launch Run simulator from build/simulator Choose \"Free Roaming\" - \"DuckieDowntown\" as map - \"Duckiebot-duckietown-ros1\" as robot make sure it's connected, click \"RUN\" - make sure it's running, you can operate the robot Run rviz or rqt_image_view and see image from topic \"/simulator/camera_node/image/compressed\"","title":"Instructions to build standalone executable"},{"location":"contributing/","text":"Contributing to LGSVL Simulator As an open project and community, we welcome and highly encourage contributions back to LGSVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of LGSVL Simulator, you can help contribute to the LGSVL Simulator project in several different ways. Feedback, questions, bug reports, and issues The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com . Submitting a Pull Request We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms. Documentation We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website. Demonstrations If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application.","title":"Contributing"},{"location":"contributing/#contributing-to-lgsvl-simulator","text":"As an open project and community, we welcome and highly encourage contributions back to LGSVL Simulator. Through feedback, questions, bug reports/issues, new features, documentation, or demonstrations showing your use case of LGSVL Simulator, you can help contribute to the LGSVL Simulator project in several different ways.","title":"Contributing to LGSVL Simulator"},{"location":"contributing/#feedback-questions-bug-reports-and-issues","text":"The best way to give feedback, raise an issue, or ask a question about LGSVL Simulator is to submit a Github issue to our main Github repository: https://www.github.com/lgsvl/simulator . Please also check first if your question has already been answered in an existing issue or on our Frequently Asked Questions page. We will try to respond to open issues as quickly as possible. If you would like to get in touch about partnerships, licensing, or larger collaborations, please email us at contact@lgsvlsimulator.com .","title":"Feedback, questions, bug reports, and issues"},{"location":"contributing/#submitting-a-pull-request","text":"We welcome pull requests for new features or bug fixes to LGSVL Simulator. You can submit a pull request here . After submitting the initial pull request, you will be prompted to agree to our Contributor License Agreement . Your pull request will then be ready for review and merging. Below are a few notes/guidelines on submitting pull requests and working on our project. While we are an open project, please be aware that we have a custom license . You must agree to the terms of our license to use and contribute back to our project. Please make sure that any preexisting licenses/terms of your contributions are compatible with our terms. Under the terms of the Unity Asset Store Terms of Service and EULA , you cannot contribute back to our project anything which you have obtained from the Unity Asset Store (unless you have separate agreements in place with Unity). Our team synchronizes the lgsvl/simulator repository internally first, then pushes to Github approximately once a week. When we merge pull requests, it will first be merged into our internal source, then pushed to Github, so there may be a slight delay for your merged PR to show up in the public repository. We always rebase and do not merge branches - when submitting a pull request, please make sure your code is rebased on the latest version of the master branch. If you are working on a large feature or a big change to how the simulator works, please submit a Github issue first or reach out to us to discuss the idea before you start working on it or submit a pull request. LGSVL Simulator is cross platform. Please do not write platform-specific code, or if you do, please make sure you do so while supporting all of our supported platforms.","title":"Submitting a Pull Request"},{"location":"contributing/#documentation","text":"We currently use mkdocs version 1.0.4 to build our documentation website . To submit a piece of documentation, please submit a PR to the lgsvl/simulator repository, placing your materials in the Docs directory. After merging, our team will build and upload to the public website.","title":"Documentation"},{"location":"contributing/#demonstrations","text":"If you are using the LGSVL Simulator for your development, we would like to hear from you. We are also always excited to see any demonstration videos, blog posts, or articles showing use cases for the LGSVL Simulator. Please reach out to us at contact@lgsvlsimulator.com to let us know about your application.","title":"Demonstrations"},{"location":"create-ros2-ad-stack/","text":"How to create a ROS2-based AD stack with LGSVL Simulator This documentation describes how to develop ROS2 nodes to receive sensor data from LGSVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with LGSVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously. Table of Contents Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package Building a ROS2 Package Running Rosbridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to LGSVL Simulator Running ROS2 Node References Requirements Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator Setup Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away. Installing Docker CE To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps . Installing NVIDIA Docker Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo . Pulling Docker Image docker pull lgsvl/lanefollowing:latest What's inside Docker Image Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook Creating a ROS2 Package A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml setup.py from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, ) package.xml ?xml version= 1.0 ? package format= 2 name lane_following /name version 0.0.1 /version description ROS2-based End-to-End Lane Following model /description maintainer email= david.uhm@lge.com David Uhm /maintainer license BSD /license exec_depend rclpy /exec_depend exec_depend std_msgs /exec_depend exec_depend sensor_msgs /exec_depend test_depend ament_copyright /test_depend test_depend ament_flake8 /test_depend test_depend ament_pep257 /test_depend test_depend python3-pytest /test_depend export build_type ament_python /build_type /export /package Building a ROS2 Package Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install Running Rosbridge Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge Writing ROS2 Subscriber Node ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously. Subscribe to a single topic import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Subscribe to multiple topics simultaneously In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main() Writing ROS2 Publisher Node The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge. Publish command back to LGSVL Simulator import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main() Running ROS2 Node Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node} References Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters","title":"How to create a simple ROS2-based AD stack"},{"location":"create-ros2-ad-stack/#how-to-create-a-ros2-based-ad-stack-with-lgsvl-simulator","text":"This documentation describes how to develop ROS2 nodes to receive sensor data from LGSVL Simulator and send control commands to drive a car. The Lane Following model is a ROS2 -based Autonomous Driving stack developed with LGSVL Simulator . In high-level overview, the model is composed of three modules: a sensor module, a perception module, and a control module. The sensor module receives raw sensor data such as camera images from the simulator and preprocess the data before feeding into the perception module. Then, the perception module takes in the preprocessed data, extracts lane information, and predicts steering wheel commands. Finally, the control module sends a predicted control command back to the simulator, which would drive a car autonomously.","title":"How to create a ROS2-based AD stack with LGSVL Simulator"},{"location":"create-ros2-ad-stack/#table-of-contents","text":"Requirements Setup Installing Docker CE Installing NVIDIA Docker Pulling Docker Image What's inside Docker Image Creating a ROS2 Package Building a ROS2 Package Running Rosbridge Writing ROS2 Subscriber Node Subscribe to a single topic Subscribe to multiple topics simultaneously Writing ROS2 Publisher Node Publish command back to LGSVL Simulator Running ROS2 Node References","title":"Table of Contents"},{"location":"create-ros2-ad-stack/#requirements","text":"Docker Python3 ROS2 TensorFlow, Keras LGSVL Simulator","title":"Requirements"},{"location":"create-ros2-ad-stack/#setup","text":"Our AD stack implementation is based on ROS2 and uses rosbridge to communicate with LGSVL Simulator. To do that, we need to prepare Ubuntu machine with ROS2 installed. We provide a Docker image containing Ubuntu 18.04 and ROS2 installed so you can just pull the Docker image and start writing code right away.","title":"Setup"},{"location":"create-ros2-ad-stack/#installing-docker-ce","text":"To install Docker CE please refer to the official documentation . We also suggest following through with the post installation steps .","title":"Installing Docker CE"},{"location":"create-ros2-ad-stack/#installing-nvidia-docker","text":"Before installing nvidia-docker make sure that you have an appropriate NVIDIA driver installed. To test if NVIDIA drivers are properly installed enter nvidia-smi in a terminal. If the drivers are installed properly an output similar to the following should appear. +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.87 Driver Version: 390.87 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A | | 0% 59C P5 22W / 250W | 1490MiB / 11175MiB | 4% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1187 G /usr/lib/xorg/Xorg 863MiB | | 0 3816 G /usr/bin/gnome-shell 305MiB | | 0 4161 G ...-token=7171B24E50C2F2C595566F55F1E4D257 68MiB | | 0 4480 G ...quest-channel-token=3330599186510203656 147MiB | | 0 17936 G ...-token=5299D28BAAD9F3087B25687A764851BB 103MiB | +-----------------------------------------------------------------------------+ The installation steps for nvidia-docker are available at the official repo .","title":"Installing NVIDIA Docker"},{"location":"create-ros2-ad-stack/#pulling-docker-image","text":"docker pull lgsvl/lanefollowing:latest","title":"Pulling Docker Image"},{"location":"create-ros2-ad-stack/#whats-inside-docker-image","text":"Ubuntu 18.04 CUDA 9.2 cuDNN 7.1.4.18 Python 3.6 TensorFlow 1.8 Keras 2.2.4 ROS1 melodic + rosbridge ROS2 Crystal + rosbridge Jupyter Notebook","title":"What's inside Docker Image"},{"location":"create-ros2-ad-stack/#creating-a-ros2-package","text":"A ROS2 package is simply a directory and should contain files named package.xml and setup.py . Create folders as below and create setup.py and package.xml . Please note that the package name must match with the folder name of your ROS package. ros2_ws/ src/ lane_following/ setup.py package.xml","title":"Creating a ROS2 Package"},{"location":"create-ros2-ad-stack/#setuppy","text":"from setuptools import setup package_name = 'lane_following' setup( name=package_name, version='0.0.1', packages=[ 'train', ], py_modules=[ 'collect', 'drive', ], install_requires=['setuptools'], zip_safe=True, author='David Uhm', author_email='david.uhm@lge.com', maintainer='David Uhm', maintainer_email='david.uhm@lge.com', keywords=[ 'ROS', 'ROS2', 'deep learning', 'lane following', 'end to end', 'LGSVL Simulator', 'Autonomous Driving' ], classifiers=[ 'Intended Audience :: Developers', 'Programming Language :: Python', 'Topic :: Software Development', ], description='ROS2-based End-to-End Lane Following model', license='BSD', tests_require=['pytest'], entry_points={ 'console_scripts': [ 'collect = collect:main', 'drive = drive:main', ], }, )","title":"setup.py"},{"location":"create-ros2-ad-stack/#packagexml","text":"?xml version= 1.0 ? package format= 2 name lane_following /name version 0.0.1 /version description ROS2-based End-to-End Lane Following model /description maintainer email= david.uhm@lge.com David Uhm /maintainer license BSD /license exec_depend rclpy /exec_depend exec_depend std_msgs /exec_depend exec_depend sensor_msgs /exec_depend test_depend ament_copyright /test_depend test_depend ament_flake8 /test_depend test_depend ament_pep257 /test_depend test_depend python3-pytest /test_depend export build_type ament_python /build_type /export /package","title":"package.xml"},{"location":"create-ros2-ad-stack/#building-a-ros2-package","text":"Now, you can build your package as below: source /opt/ros/crystal/setup.bash cd ~/ros2_ws colcon build --symlink-install","title":"Building a ROS2 Package"},{"location":"create-ros2-ad-stack/#running-rosbridge","text":"Rosbridge provides a JSON API to ROS functionality for non-ROS programs such as LGSVL Simulator. You can run rosbridge to connect your ROS node with LGSVL Simulator as below: source /opt/ros/crystal/setup.bash rosbridge","title":"Running Rosbridge"},{"location":"create-ros2-ad-stack/#writing-ros2-subscriber-node","text":"ROS nodes communicate with each other by passing messages. Messages are routed via a topic with publish/subscribe concepts. A node sends out a message by publishing it to a given topic. Then, a node that is interested in a certain kind of data will subscribe to the appropriate topic. In our cases, LGSVL Simulator publishes sensor data such as camera images or LiDAR point clouds via rosbridge, and then the Lane Following model subscribes to that topic to receive sensor data, preprocesses the data, feeds them into the pretrained model, and finally computes a control command based on perceived sensor data. Below is an example of how to subscribe to sensor data topics from a ROS node. You can subscribe to a single topic only or multiple topics simultaneously.","title":"Writing ROS2 Subscriber Node"},{"location":"create-ros2-ad-stack/#subscribe-to-a-single-topic","text":"import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage class Drive(Node): def __init__(self): super().__init__('drive') self.sub_image = self.create_subscription(CompressedImage, '/simulator/sensor/camera/center/compressed', self.callback) def callback(self, msg): self.get_logger().info('Subscribed: {}'.format(msg.data)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Subscribe to a single topic"},{"location":"create-ros2-ad-stack/#subscribe-to-multiple-topics-simultaneously","text":"In order to subscribe to ROS messages of different types from multiple sources, we need to take the timestamps of those messages into account. ROS2 Message Filters is the ROS package that synchronizes incoming messages by the timestamps contained in their headers and outputs them in the form of a single callback. Install this package in your ROS2 workspace and build it. cd ~/ros2_ws/src git clone https://github.com/intel/ros2_message_filters.git cd .. colcon build --symlink-install Then, you can import it in your python script by import message_filters and use it as below: import rclpy from rclpy.node import Node from sensor_msgs.msg import CompressedImage from geometry_msgs.msg import TwistStamped import message_filters class Collect(Node): def __init__(self): super().__init__('collect') sub_center_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/center/compressed') sub_left_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/left/compressed') sub_right_camera = message_filters.Subscriber(self, CompressedImage, '/simulator/sensor/camera/right/compressed') sub_control = message_filters.Subscriber(self, TwistStamped, '/simulator/control/command') ts = message_filters.ApproximateTimeSynchronizer([sub_center_camera, sub_left_camera, sub_right_camera, sub_control], 1, 0.1) ts.registerCallback(self.callback) def callback(self, center_camera, left_camera, right_camera, control): self.get_logger().info('Subscribed: {}'.format(control.twist.angular.x)) def main(args=None): rclpy.init(args=args) collect = Collect() rclpy.spin(collect) if __name__ == '__main__': main()","title":"Subscribe to multiple topics simultaneously"},{"location":"create-ros2-ad-stack/#writing-ros2-publisher-node","text":"The publisher sends data to a topic. When you create a publisher you have to tell ROS of which type the data will be. In order to drive a car autonomously, the Lane Following model publishes a predicted control command back to the simulator via rosbridge.","title":"Writing ROS2 Publisher Node"},{"location":"create-ros2-ad-stack/#publish-command-back-to-lgsvl-simulator","text":"import rclpy from rclpy.node import Node from geometry_msgs.msg import TwistStamped class Drive(Node): def __init__(self): super().__init__('drive') self.control_pub = self.create_publisher(TwistStamped, '/lanefollowing/steering_cmd') self.timer_period = 0.02 # seconds self.timer = self.create_timer(self.timer_period, self.callback) self.steering = 0. def callback(self): message = TwistStamped() message.twist.angular.x = float(self.steering) self.control_pub.publish(message) self.get_logger().info('Publishing: {}'.format(message.twist.angular.x)) def main(args=None): rclpy.init(args=args) drive = Drive() rclpy.spin(drive) if __name__ == '__main__': main()","title":"Publish command back to LGSVL Simulator"},{"location":"create-ros2-ad-stack/#running-ros2-node","text":"Once you have setup the rosbridge connection to LGSVL Simulator, you can launch your ROS node as follows: source /opt/ros/crystal/setup.bash source ~/ros2_ws/install/local_setup.bash ros2 run {your_package} {your_node}","title":"Running ROS2 Node"},{"location":"create-ros2-ad-stack/#references","text":"Lane Following Github Repository LGSVL Simulator ROS2 Documentation ROS2 Message Filters","title":"References"},{"location":"faq/","text":"LGSVL Simulator FAQ What are the recommended system specs? What are the minimum REQUIRED system specs? For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz quad core CPU, Nvidia GTX 1080 graphics card, and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, Nvidia graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and nvidia drivers provide better performance on Windows than on Linux. Does the simulator run on Windows/Mac/Linux? Officially, you can run LGSVL Simulator on Windows 10 and Ubuntu 16.04 (or later). We do not support macOS at this time. Which Unity version is required and how do I get it? LGSVL Simulator is currently on Unity version 2018.2.4, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (2018.2.4f1) here: https://beta.unity3d.com/download/fe703c5165de/public_download.html We are constantly working to ensure that LGSVL Simulator runs on the latest version of Unity which supports all of our required functionality. Why are assets/scenes missing/empty after cloning from git? We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name WebSocketSharp could not be found. Are you missing a using directive or an assembly reference? Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo? If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git ROS Bridge won't connect? First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports. How do I control the ego vehicle (my vehicle) spawn position? Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component. How can I add a custom ego vehicle to LGSVL Simulator? Please see our tutorial on how to add a new ego vehicle to LGSVL Simulator here . How can I add extra sensors to vehicles in LGSVL Simulator? To add extra camera, or lidar you can copy paste existing sensor in vehicle prefab. Then you can adjust its parameters. As last step you need to add sensor component to list of active sensors in vehicle setup component. See instruction in GitHub issue #68 for how to add extra camera sensor. How can I add a custom map to LGSVL Simulator? Locate DefaultAssetBundleSettings.asset in Project window and add the new map .scene to the Maps list and optionally map image. You cannot reuse map image files for different scenes. To have scene available inside Unity Editor, add the new scene to build settings. How can I create or edit map annotations? Please see our tutorial on how to add map annotations in LGSVL Simulator here . Why are pedestrians not spawning when annotated correctly? LGSVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window - AI - Navigation and bake the NavMesh. Why can't I find catkin_make command when building Apollo? Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo. Why is Apollo perception module turning on and off all the time? This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo Why does the Apollo vehicle stop at stop line and not cross intersections? Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED). Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\" This is expected behavior. LGSVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it. Why does Rviz not load the Autoware vector map? Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516 Why are there no maps when I make a local build? Default File - Build... menu will not work inside Unity Editor. Simulator requires running extra steps to correctly produce working map. On Linux you can do the build with following command: mkdir build /opt/Unity/Editor/Unity \\ -batchmode \\ -nographics \\ -silent-crashes \\ -quit \\ -buildDestination ./build/simulator \\ -buildTarget Linux64 \\ -executeMethod BuildScript.Build \\ -projectPath . \\ -logFile /dev/stdout On Windows you can do the build with following command: mkdir build C:\\Program Files\\Unity\\Editor\\Unity.exe ^ -batchmode ^ -nographics ^ -silent-crashes ^ -quit ^ -buildDestination build\\simulator.exe ^ -buildTarget Windows64 ^ -executeMethod BuildScript.Build ^ -projectPath . You need to run these commands from folder where project is cloned. Unity must be closed when build is running. Other questions? See our Github issues page, or email us at contact@lgsvlsimulator.com .","title":"FAQ"},{"location":"faq/#lgsvl-simulator-faq","text":"","title":"LGSVL Simulator FAQ"},{"location":"faq/#what-are-the-recommended-system-specs-what-are-the-minimum-required-system-specs","text":"For optimal performance, we recommend that you run the simulator on a system with at least a 4 GHz quad core CPU, Nvidia GTX 1080 graphics card, and 16GB memory or higher, running on Windows 10. While you can run on a lower-spec system, the performance of the simulator will be impacted and you will probably see much lower frame rates. The minimum specification to run is a 3GHz dual core CPU, Nvidia graphics card, and 8 GB memory system. Note that simulator runs better on Windows due to fact that Unity and nvidia drivers provide better performance on Windows than on Linux.","title":"What are the recommended system specs? What are the minimum REQUIRED system specs?"},{"location":"faq/#does-the-simulator-run-on-windowsmaclinux","text":"Officially, you can run LGSVL Simulator on Windows 10 and Ubuntu 16.04 (or later). We do not support macOS at this time.","title":"Does the simulator run on Windows/Mac/Linux?"},{"location":"faq/#which-unity-version-is-required-and-how-do-i-get-it","text":"LGSVL Simulator is currently on Unity version 2018.2.4, and can be downloaded from the Unity Download Archive. You can download the Windows version here: https://unity3d.com/get-unity/download/archive You can download the Linux version (2018.2.4f1) here: https://beta.unity3d.com/download/fe703c5165de/public_download.html We are constantly working to ensure that LGSVL Simulator runs on the latest version of Unity which supports all of our required functionality.","title":"Which Unity version is required and how do I get it?"},{"location":"faq/#why-are-assetsscenes-missingempty-after-cloning-from-git","text":"We use Git LFS for large file storage to improve performance of cloning. Before cloning, install and run git lfs install . Then repeat the git clone process. You can find the Git LFS installation instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation Typically if you do not have Git LFS installed or configured then you will see the following error when opening Unity project: error CS026: The type or namespace name WebSocketSharp could not be found. Are you missing a using directive or an assembly reference?","title":"Why are assets/scenes missing/empty after cloning from git?"},{"location":"faq/#why-do-i-get-an-error-saying-some-files-eg-rosbridge_websocketlaunch-are-missing-in-apollo","text":"If you see that some files are missing from ros_pkgs folder in Apollo repository, you need to make sure that you are cloning all submodules: git clone --recurse-submodules https://github.com/lgsvl/apollo.git","title":"Why do I get an error saying some files (e.g. rosbridge_websocket.launch) are missing in Apollo?"},{"location":"faq/#ros-bridge-wont-connect","text":"First make sure you are running rosbridge. If using our Apollo docker image, run: rosbridge.sh For standalone ROS environments run: roslaunch rosbridge_server rosbridge_websocket.launch If you are running ROS bridge on different machine, verify that simulator can connect to it and you do not have firewall blocking ports.","title":"ROS Bridge won't connect?"},{"location":"faq/#how-do-i-control-the-ego-vehicle-my-vehicle-spawn-position","text":"Find the \"spawn_transform\" game objects in scene and adjust their transform position. If you are creating new maps make sure you add \"SpawnInfo\" component to empty game object. The Simulator will use location of first game object that has SpawnInfo component.","title":"How do I control the ego vehicle (my vehicle) spawn position?"},{"location":"faq/#how-can-i-add-a-custom-ego-vehicle-to-lgsvl-simulator","text":"Please see our tutorial on how to add a new ego vehicle to LGSVL Simulator here .","title":"How can I add a custom ego vehicle to LGSVL Simulator?"},{"location":"faq/#how-can-i-add-extra-sensors-to-vehicles-in-lgsvl-simulator","text":"To add extra camera, or lidar you can copy paste existing sensor in vehicle prefab. Then you can adjust its parameters. As last step you need to add sensor component to list of active sensors in vehicle setup component. See instruction in GitHub issue #68 for how to add extra camera sensor.","title":"How can I add extra sensors to vehicles in LGSVL Simulator?"},{"location":"faq/#how-can-i-add-a-custom-map-to-lgsvl-simulator","text":"Locate DefaultAssetBundleSettings.asset in Project window and add the new map .scene to the Maps list and optionally map image. You cannot reuse map image files for different scenes. To have scene available inside Unity Editor, add the new scene to build settings.","title":"How can I add a custom map to LGSVL Simulator?"},{"location":"faq/#how-can-i-create-or-edit-map-annotations","text":"Please see our tutorial on how to add map annotations in LGSVL Simulator here .","title":"How can I create or edit map annotations?"},{"location":"faq/#why-are-pedestrians-not-spawning-when-annotated-correctly","text":"LGSVL Simulator uses Unity's NavMesh API to work correctly. In Unity Editor, select Window - AI - Navigation and bake the NavMesh.","title":"Why are pedestrians not spawning when annotated correctly?"},{"location":"faq/#why-cant-i-find-catkin_make-command-when-building-apollo","text":"Make sure you are not running Apollo dev_start/into.sh scripts as root. The will not work as root. You need to run them as non-root user, without sudo.","title":"Why can't I find catkin_make command when building Apollo?"},{"location":"faq/#why-is-apollo-perception-module-turning-on-and-off-all-the-time","text":"This means that Apollo perception process is exiting with error. Check apollo/data/log/perception.ERROR file for error messages. Typically you will see following error: Check failed: error == cudaSuccess (8 vs. 0) invalid device function This means one of two things: 1) GPU you are using is not supported by Apollo. Apollo requires CUDA 8 compatible hardware, it won't work if GPU is too old or too new. Apollo officially supports only GTX 1080. RTX 2080 will not work. 2) Other option is that CUDA driver is broken. To fix this you will need to restart your computer. Check that CUDA works on your host system by running one of CUDA examples before running Apollo","title":"Why is Apollo perception module turning on and off all the time?"},{"location":"faq/#why-does-the-apollo-vehicle-stop-at-stop-line-and-not-cross-intersections","text":"Apollo vehicle continues over intersection only when traffic light is green. If perception module does not see traffic light, the vehicle won't move. Check previous question to verify that perception module is running and Apollo is seeing traffic light (top left of dreamview should say GREEN or RED).","title":"Why does the Apollo vehicle stop at stop line and not cross intersections?"},{"location":"faq/#dreamview-in-apollo-shows-hardware-gps-triggers-safety-mode-no-gnss-status-message","text":"This is expected behavior. LGSVL Simulator does simulation on software level. It sends only ROS messages to Apollo. Dreamview in Apollo has extra checks that tries to verify if hardware devices are working correctly and are not disconnected. This error message means that Apollo does not see GPS hardware working (as it is not present). It it safe to ignore it.","title":"Dreamview in Apollo shows \"Hardware GPS triggers safety mode. No GNSS status message.\""},{"location":"faq/#why-does-rviz-not-load-the-autoware-vector-map","text":"Loading SanFrancisco map in Rviz for Autoware is a very slow process, because SanFrancisco map has many annotations and Rviz cannot handle them efficiently. It will either crash or will take many minutes if not hours. You can checkout older commit of autoware-data repository that has annotations only for smaller part of SanFrancisco. git checkout e3cfe709e4af32ad2ea8ea4de85579b9916fe516","title":"Why does Rviz not load the Autoware vector map?"},{"location":"faq/#why-are-there-no-maps-when-i-make-a-local-build","text":"Default File - Build... menu will not work inside Unity Editor. Simulator requires running extra steps to correctly produce working map. On Linux you can do the build with following command: mkdir build /opt/Unity/Editor/Unity \\ -batchmode \\ -nographics \\ -silent-crashes \\ -quit \\ -buildDestination ./build/simulator \\ -buildTarget Linux64 \\ -executeMethod BuildScript.Build \\ -projectPath . \\ -logFile /dev/stdout On Windows you can do the build with following command: mkdir build C:\\Program Files\\Unity\\Editor\\Unity.exe ^ -batchmode ^ -nographics ^ -silent-crashes ^ -quit ^ -buildDestination build\\simulator.exe ^ -buildTarget Windows64 ^ -executeMethod BuildScript.Build ^ -projectPath . You need to run these commands from folder where project is cloned. Unity must be closed when build is running.","title":"Why are there no maps when I make a local build?"},{"location":"faq/#other-questions","text":"See our Github issues page, or email us at contact@lgsvlsimulator.com .","title":"Other questions?"},{"location":"getting-started/","text":"LGSVL Simulator: An Autonomous Vehicle Simulator Website | Documentation | Download Introduction LG Silicon Valley Lab has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the Duckietown , TierIV's Autoware , and Baidu's Apollo platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork . Video ( Link ) Getting Started Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Dual core CPU Nvidia GTX 1080 Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux. Downloading and starting simulator Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Unzip the downloaded folder and run the executable. Steps for starting simulator in Unity Editor Install Unity 2018.2.4. Windows link: https://unity3d.com/get-unity/download/archive Linux link (2018.2.4f1): https://beta.unity3d.com/download/fe703c5165de/public_download.html Install Git LFS (this should be as simple as git lfs install ). Since this repository contains several large files, we speed up clones/uploads with Git LFS. This is a requirement for cloning our project, and without it the repository cannot be set up properly. Clone this repository from Github: git clone https://github.com/lgsvl/simulator.git Open Unity Editor by navigating to where it is installed and launching the Unity executable. Open the simulator project that was cloned in step 3 by selecting the simulator folder. Run the simulator. On the bottom left in the Project panel, Navigate to Assets- Scenes. Then double-click \"Menu\" (with the Unity icon next to it) to load the scene. At the top of the editor, click the Play button to start the simulator. Build standalone executable If you would prefer to not run in Unity Editor and build the standalone executable yourself, follow the instructions here . Simulator Instructions After starting the simulator, you should see the main menu. Currently, only Free Roaming mode is supported. Click \"Free Roaming.\" Select the appropriate map and vehicle. For a standard setup, select \"SanFrancisco\" for map and \"XE_Rigged-apollo\" for Robot. If connecting with Autoware or Apollo, make sure simulator establishes connection with rosbridge. Click \"Run\" to begin. The program will not allow running if there is no established connection with a rosbridge. To bypass this and just test out the simulator environment, hold down the Shift button and click \"Run.\" The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo repositories for instructions on running the platforms with the simulator. Guide to simulator functionality Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator. Contact Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at . Copyright and License Copyright (c) 2018 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Getting started"},{"location":"getting-started/#introduction","text":"LG Silicon Valley Lab has developed a Unity-based multi-robot simulator for autonomous vehicle developers. We provide an out-of-the-box solution which can meet the needs of developers wishing to focus on testing their autonomous vehicle algorithms. It currently has integration with the Duckietown , TierIV's Autoware , and Baidu's Apollo platforms, can generate HD maps, and be immediately used for testing and validation of a whole system with little need for custom integrations. We hope to build a collaborative community among robotics and autonomous vehicle developers by open sourcing our efforts. To use the simulator with Apollo, after following the build steps for the simulator, follow the guide on our Apollo fork . To use the simulator with Autoware, build the simulator then follow the guide on our Autoware fork .","title":"Introduction"},{"location":"getting-started/#video","text":"( Link )","title":"Video"},{"location":"getting-started/#getting-started","text":"Running the simulator with reasonable performance and frame rate (for perception related tasks) requires a high performance desktop. Below is the recommended system for running the simulator at high quality. We are currently working on performance improvements for a better experience. Recommended system: 4 GHz Dual core CPU Nvidia GTX 1080 Windows 10 64 Bit The easiest way to get started with running the simulator is to download our latest release and run as a standalone executable. For the latest functionality or if you want to modify the simulator for your own needs, you can checkout our source, open it as a project in Unity, and run inside the Unity Editor. Otherwise, you can build the Unity project into a standalone executable. Currently, running the simulator in Windows yields better performance than running on Linux.","title":"Getting Started"},{"location":"getting-started/#downloading-and-starting-simulator","text":"Download the latest release of the LGSVL Simulator for your supported operating system (Windows or Linux) here: https://github.com/lgsvl/simulator/releases/latest Unzip the downloaded folder and run the executable.","title":"Downloading and starting simulator"},{"location":"getting-started/#steps-for-starting-simulator-in-unity-editor","text":"Install Unity 2018.2.4. Windows link: https://unity3d.com/get-unity/download/archive Linux link (2018.2.4f1): https://beta.unity3d.com/download/fe703c5165de/public_download.html Install Git LFS (this should be as simple as git lfs install ). Since this repository contains several large files, we speed up clones/uploads with Git LFS. This is a requirement for cloning our project, and without it the repository cannot be set up properly. Clone this repository from Github: git clone https://github.com/lgsvl/simulator.git Open Unity Editor by navigating to where it is installed and launching the Unity executable. Open the simulator project that was cloned in step 3 by selecting the simulator folder. Run the simulator. On the bottom left in the Project panel, Navigate to Assets- Scenes. Then double-click \"Menu\" (with the Unity icon next to it) to load the scene. At the top of the editor, click the Play button to start the simulator.","title":"Steps for starting simulator in Unity Editor"},{"location":"getting-started/#build-standalone-executable","text":"If you would prefer to not run in Unity Editor and build the standalone executable yourself, follow the instructions here .","title":"Build standalone executable"},{"location":"getting-started/#simulator-instructions","text":"After starting the simulator, you should see the main menu. Currently, only Free Roaming mode is supported. Click \"Free Roaming.\" Select the appropriate map and vehicle. For a standard setup, select \"SanFrancisco\" for map and \"XE_Rigged-apollo\" for Robot. If connecting with Autoware or Apollo, make sure simulator establishes connection with rosbridge. Click \"Run\" to begin. The program will not allow running if there is no established connection with a rosbridge. To bypass this and just test out the simulator environment, hold down the Shift button and click \"Run.\" The vehicle/robot should spawn inside the map environment that was selected. Read here for an explanation of all current keyboard shortcuts and controls. Follow the guides on our respective Autoware and Apollo repositories for instructions on running the platforms with the simulator.","title":"Simulator Instructions"},{"location":"getting-started/#guide-to-simulator-functionality","text":"Look here for a guide to currently available functionality and keyboard shortcuts for using the simulator.","title":"Guide to simulator functionality"},{"location":"getting-started/#contact","text":"Please feel free to provide feedback or ask questions by creating a Github issue. For inquiries about collaboration, please email us at .","title":"Contact"},{"location":"getting-started/#copyright-and-license","text":"Copyright (c) 2018 LG Electronics, Inc. This software contains code licensed as described in LICENSE.","title":"Copyright and License"},{"location":"keyboard-shortcuts/","text":"Simulator Controls Key Bindings Officially supported: Esc - Exit prompt F1 - Help menu Space - Toggle User Interface - Drive vehicle/robot forward/back, turn For developer use: F5 - Save current position of vehicle F9 - Reset to last saved position of vehicle F12 - Toggle weather control panel H - Spawn/Respawn Non-Player Character (NPC) vehicles K - Remove NPC vehicles End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear V - Toggle main camera view (from in front of car, from inside of car) Miscellaneous Left Shift - Toggle headlights Right Shift - Toggle parking brake (needs to be off for vehicle to move) Num Pad 5 - Num Pad 9 - Windshield wipers Steering Wheel Controls (Demonstration) We use the Logitech G920 steering wheel with our simulator. The following table shows key mappings on the wheel for demonstration purposes, mainly for developer use. Logitech Wheel Button Keyboard equivalent Effect Right paddle Up Accelerate Left paddle Down Brake RSB Page Up/ Page Down Toggle between Drive and Reverse LSB Num 5-8 Toggle windshield wipers on/off A Spacebar Toggle UI Switch (sensor menu camera display) B H Respawn all NPC traffic (not toggle) X - Toggle sensor effects (LiDAR) Y V Cycle through camera views D-pad Up - Daytime scenario (no rain or fog) D-pad Down - Rainy scenario (with fog) D-pad Left - Night scenario (rain + fog + road wetness) D-pad Right - Sunrise foggy scenario Overlapping Squares Left Shift Cycle through headlight settings (off-low-high) Hamburger Menu (3 horizontal lines) Right Shift Toggle parking brake Xbox End Toggle vehicle ignition","title":"Keyboard shortcuts"},{"location":"keyboard-shortcuts/#simulator-controls","text":"","title":"Simulator Controls"},{"location":"keyboard-shortcuts/#key-bindings","text":"","title":"Key Bindings"},{"location":"keyboard-shortcuts/#officially-supported","text":"Esc - Exit prompt F1 - Help menu Space - Toggle User Interface - Drive vehicle/robot forward/back, turn","title":"Officially supported:"},{"location":"keyboard-shortcuts/#for-developer-use","text":"F5 - Save current position of vehicle F9 - Reset to last saved position of vehicle F12 - Toggle weather control panel H - Spawn/Respawn Non-Player Character (NPC) vehicles K - Remove NPC vehicles End - Toggle ignition (must be on for vehicle to move) Page Up - Shift to forward drive gear Page Down - Shift to reverse gear V - Toggle main camera view (from in front of car, from inside of car)","title":"For developer use:"},{"location":"keyboard-shortcuts/#miscellaneous","text":"Left Shift - Toggle headlights Right Shift - Toggle parking brake (needs to be off for vehicle to move) Num Pad 5 - Num Pad 9 - Windshield wipers","title":"Miscellaneous"},{"location":"keyboard-shortcuts/#steering-wheel-controls-demonstration","text":"We use the Logitech G920 steering wheel with our simulator. The following table shows key mappings on the wheel for demonstration purposes, mainly for developer use. Logitech Wheel Button Keyboard equivalent Effect Right paddle Up Accelerate Left paddle Down Brake RSB Page Up/ Page Down Toggle between Drive and Reverse LSB Num 5-8 Toggle windshield wipers on/off A Spacebar Toggle UI Switch (sensor menu camera display) B H Respawn all NPC traffic (not toggle) X - Toggle sensor effects (LiDAR) Y V Cycle through camera views D-pad Up - Daytime scenario (no rain or fog) D-pad Down - Rainy scenario (with fog) D-pad Left - Night scenario (rain + fog + road wetness) D-pad Right - Sunrise foggy scenario Overlapping Squares Left Shift Cycle through headlight settings (off-low-high) Hamburger Menu (3 horizontal lines) Right Shift Toggle parking brake Xbox End Toggle vehicle ignition","title":"Steering Wheel Controls (Demonstration)"},{"location":"map-annotation/","text":"Map Annotation The LGSVL Simulator supports creating, editing, and exporting of HD/vector maps of existing 3D environments (Unity scenes). The maps can be saved in the currently supported Apollo or Autoware formats. Currently, map annotation is only supported while running the simulator as a Unity project in a Windows environment. Creating a new map Open MapToolUtilEdit in Unity : Window - Map Tool Panel By default, map annotation is not shown. Click Show/Hide Map to show existing map annotation. Before annotation, select correct Parent Object in MapToolUtilEdit, for example Map . Then every new object you create will be under Map object. - After annotation is done, remember to save: go to Map level, click Apply Annotate Lanes Create temp map waypoint Maker sure your roads belong to the layer of Ground and Road since waypoint will be only created on this layer. Click Create Temp Map Waypoint button to create a new point in the center of current scene window. Make Lane Select the waypoint, ctrl + d to duplicate and move them to desired positions - Connect them to make a lane - Sequentially select points - Click Make Lane (MapLaneSegmentBuilder) button to make a lane - You can make StopLine and BoundaryLine in a similar way by using the other two buttons below Make Lane (MapLaneSegmentBuilder) In-Between Lane Generation You can make lanes by creating temp points and connecting them, and you can also make an in-between lane easily when you have two existing lanes and want to make another lane to connect them together Select both lanes, create the new in-between lane by clicking Auto Generate In-Between Lane button at the bottom of MapToolUtilEdit panel Because you might want to connect lanes of different angles and do it multiple times, we provide the ability of saving different presets to generate in-between lanes. Larger value of Tangent makes the lane more straight You can also change the number of presets by changing Preset Count Check Offset Start/End Points if you want a small offset between the end points of the new lane and the end point of the selected two lanes, otherwise those points will overlap with each other completely. Link neighbor lanes After you create parallel lanes, you need to link them to have correct relations Link neighbor lanes of same direction Select lanes sequentially from left/right Click Link Neighbor Lanes from Left/Right to link them After linking, you can check it in every lane object's inspector Link reverse neighbor lanes Select both reverse neighbor lanes Click Link Reverse Neighbor Lanes In the above screenshot, double yellow boundary lane can be generated by creating BoundaryLine and changing its Line Type from SOLID_WHITE to DOUBLE_YELLOW in its inspector If anything goes wrong, you can nullify all neighbor lane fields by clicking Nullify All Neighbor Lane Fields button Set correct left/right boundary type The rightmost lane typically has a right boundary type of CURB Middle lanes typically have a left boundary type of DOUBLE_YELLOW Traffic light annotation Make SignalLight Select local instead of global in both Unity and MapToolUtilEdit panel Find one traffic light that is already made whose type is HDMapSignalLight (with a red boundary box) Load template by clicking Load SignalLight Template button In the above screenshot, the left SignalLight (type: HDMapSignalLight) has a red boundary box and the right one (type: MapSignalLight) doesn't have it Make sure relative axis relations are selected correctly In the row of Make SignalLight(HDMapSignalLightBuilder) , select Aim as Y Neg , Up as Z Pos Select your desired empty signal light holder Click Make SignalLight (HDMapSignalLightBuilder) to generate the signal light Note: currently, HDMapSignalLightBuilder is for Apollo; one per road is enough. MapSignalLightBuilder is for Autoware, which needs one for every lane. There is a converter button at the bottom of MapToolUtilEdit to get MapSignalLightBuilder from HDMapSignalLightBuilder Link the created traffic light to the corresponding stopline Select both signallight and the stopline Click Link SignalLight and StopLine in Advanced Utils section in the panel Similarly, you can create stop sign and link the stop sign to the corresponding stopline using corresponding buttons. Create traffic pole and link to contained signal light (additional step for Autoware) Select desired traffic pole Make sure Local and Z Pos are selected Click Make Traffic Pole (VectorMapPoleBuilder) Press w so you can rotate the created pole. Rotate the bounding box of the pole to include corresponding signal lights in it Click Link to Contained SignalLights in the traffic pole's inspector OR select both signal lights and traffic pole, click Link TrafficPole and SignalLight in the MapToolUtilEdit panel For traffic poles in multiple intersections, to save time you can Create and rotate for all VectorMapPole s Select all VectorMapPole in Hierarchy tab - Link Selected TrafficPoles To Contained SignalLights Notes for stopline For Autoware, there needs to be two waypoints for every lane, i.e. five waypoints are needed for four lanes, three waypoints are needed for two lanes, etc. Apollo does not have such a requirement. Export map files For Apollo, select HDMapTool in Hierachy tab, click Export HD Map If your map annotation is correct, you will see Successfully generated and exported Apollo HD Map! in the Console You can find the generated map file in hd_map folder under the root of the repo It is similar for Autoware: select VectorMapTool , then click Export Vector Map","title":"Map Annotation"},{"location":"map-annotation/#map-annotation","text":"The LGSVL Simulator supports creating, editing, and exporting of HD/vector maps of existing 3D environments (Unity scenes). The maps can be saved in the currently supported Apollo or Autoware formats. Currently, map annotation is only supported while running the simulator as a Unity project in a Windows environment.","title":"Map Annotation"},{"location":"map-annotation/#creating-a-new-map","text":"Open MapToolUtilEdit in Unity : Window - Map Tool Panel By default, map annotation is not shown. Click Show/Hide Map to show existing map annotation. Before annotation, select correct Parent Object in MapToolUtilEdit, for example Map . Then every new object you create will be under Map object. - After annotation is done, remember to save: go to Map level, click Apply","title":"Creating a new map"},{"location":"map-annotation/#annotate-lanes","text":"","title":"Annotate Lanes"},{"location":"map-annotation/#create-temp-map-waypoint","text":"Maker sure your roads belong to the layer of Ground and Road since waypoint will be only created on this layer. Click Create Temp Map Waypoint button to create a new point in the center of current scene window.","title":"Create temp map waypoint"},{"location":"map-annotation/#make-lane","text":"Select the waypoint, ctrl + d to duplicate and move them to desired positions - Connect them to make a lane - Sequentially select points - Click Make Lane (MapLaneSegmentBuilder) button to make a lane - You can make StopLine and BoundaryLine in a similar way by using the other two buttons below Make Lane (MapLaneSegmentBuilder) In-Between Lane Generation You can make lanes by creating temp points and connecting them, and you can also make an in-between lane easily when you have two existing lanes and want to make another lane to connect them together Select both lanes, create the new in-between lane by clicking Auto Generate In-Between Lane button at the bottom of MapToolUtilEdit panel Because you might want to connect lanes of different angles and do it multiple times, we provide the ability of saving different presets to generate in-between lanes. Larger value of Tangent makes the lane more straight You can also change the number of presets by changing Preset Count Check Offset Start/End Points if you want a small offset between the end points of the new lane and the end point of the selected two lanes, otherwise those points will overlap with each other completely.","title":"Make Lane"},{"location":"map-annotation/#link-neighbor-lanes","text":"After you create parallel lanes, you need to link them to have correct relations Link neighbor lanes of same direction Select lanes sequentially from left/right Click Link Neighbor Lanes from Left/Right to link them After linking, you can check it in every lane object's inspector Link reverse neighbor lanes Select both reverse neighbor lanes Click Link Reverse Neighbor Lanes In the above screenshot, double yellow boundary lane can be generated by creating BoundaryLine and changing its Line Type from SOLID_WHITE to DOUBLE_YELLOW in its inspector If anything goes wrong, you can nullify all neighbor lane fields by clicking Nullify All Neighbor Lane Fields button","title":"Link neighbor lanes"},{"location":"map-annotation/#set-correct-leftright-boundary-type","text":"The rightmost lane typically has a right boundary type of CURB Middle lanes typically have a left boundary type of DOUBLE_YELLOW","title":"Set correct left/right boundary type"},{"location":"map-annotation/#traffic-light-annotation","text":"Make SignalLight Select local instead of global in both Unity and MapToolUtilEdit panel Find one traffic light that is already made whose type is HDMapSignalLight (with a red boundary box) Load template by clicking Load SignalLight Template button In the above screenshot, the left SignalLight (type: HDMapSignalLight) has a red boundary box and the right one (type: MapSignalLight) doesn't have it Make sure relative axis relations are selected correctly In the row of Make SignalLight(HDMapSignalLightBuilder) , select Aim as Y Neg , Up as Z Pos Select your desired empty signal light holder Click Make SignalLight (HDMapSignalLightBuilder) to generate the signal light Note: currently, HDMapSignalLightBuilder is for Apollo; one per road is enough. MapSignalLightBuilder is for Autoware, which needs one for every lane. There is a converter button at the bottom of MapToolUtilEdit to get MapSignalLightBuilder from HDMapSignalLightBuilder Link the created traffic light to the corresponding stopline Select both signallight and the stopline Click Link SignalLight and StopLine in Advanced Utils section in the panel Similarly, you can create stop sign and link the stop sign to the corresponding stopline using corresponding buttons.","title":"Traffic light annotation"},{"location":"map-annotation/#create-traffic-pole-and-link-to-contained-signal-light-additional-step-for-autoware","text":"Select desired traffic pole Make sure Local and Z Pos are selected Click Make Traffic Pole (VectorMapPoleBuilder) Press w so you can rotate the created pole. Rotate the bounding box of the pole to include corresponding signal lights in it Click Link to Contained SignalLights in the traffic pole's inspector OR select both signal lights and traffic pole, click Link TrafficPole and SignalLight in the MapToolUtilEdit panel For traffic poles in multiple intersections, to save time you can Create and rotate for all VectorMapPole s Select all VectorMapPole in Hierarchy tab - Link Selected TrafficPoles To Contained SignalLights Notes for stopline For Autoware, there needs to be two waypoints for every lane, i.e. five waypoints are needed for four lanes, three waypoints are needed for two lanes, etc. Apollo does not have such a requirement.","title":"Create traffic pole and link to contained signal light (additional step for Autoware)"},{"location":"map-annotation/#export-map-files","text":"For Apollo, select HDMapTool in Hierachy tab, click Export HD Map If your map annotation is correct, you will see Successfully generated and exported Apollo HD Map! in the Console You can find the generated map file in hd_map folder under the root of the repo It is similar for Autoware: select VectorMapTool , then click Export Vector Map","title":"Export map files"},{"location":"npc-map-navigation/","text":"NPC Map Navigation Non-player character (NPC) vehicles now use the MapSegmentBuilder classes to navigate annotated maps. Map Manager Use SanFrancisco.scene as a template to build map data for NPCs. Remove SFTraffic_New.prefab from scene and any associated scripts, e.g., TrafPerformanceManager.cs, TrafInfoManager.cs and TrafSystem.cs. Create a new GameObject at position Vector3.zero and Quaternion.identity, named Map . Add MapManager.cs component to this object and save as a prefab in project assets. The public field, SpawnLanesHolder, of MapManager.cs requires the MapLaneSegmentBuilder holder transform. The public field, IntersectionsHolder, of MapManager.cs requires the TrafficLights holder transform. This has intersection meshes and scripts for lights. The public fields Green, Yellow, and Red are materials for the segmentation camera system. If using the Traffic meshes from SanFrancisco.scene, these need to be added here. If not, IntersectionComponent.cs and associated scripts will need to be edited. Map Lane and Intersection Grouping Create TrafficLanes holder object as a child of Map.prefab . Place all MapLaneSegmentBuilder objects into TrafficLanes holder object for all non intersection lanes. Create IntersectionLanes holder object as a child of Map.prefab . Create a new Intersection holder object as a child of IntersectionLanes transform for each intersection annotation. Be sure its world position is in the center of each intersection. Place MapLaneSegmentBuilder and MapStopLineSegmentBuilder objects into Intersection holder object for each intersection. Map Intersection Builder For each Intersection holder, add the MapIntersectionBuilder.cs component. Traffic Lights Create a TrafficLights holder object to hold all traffic light meshes or place all traffic meshes under the map annotation Intersections . Just be sure to have the root holder be in MapManager.cs IntersectionHolder public reference. Create a Intersection holder object. Be sure its world position is in the center of each intersection. Add IntersectionComponent.cs to each Intersection holder object. Place TrafficLightPole facing it's corresponding StopLineSegmentBuilder object. The transfom needs to be Z axis or gizmo arrow forward, parallel to the StopLineSegmentBuilder object Z axis or gizmo arrow forward. Add IntersectionTrafficLightSetComponent.cs. Place as a child of the Intersection holder object. For opposite facing TrafficLightPoles and StopLineSegmentBuilders , be sure to orient transforms in Z axis or gizmo arrow forward but perpendicular to other facing light poles and stoplines. Add TrafficLight meshes as children of the TrafficLightPole . Add IntersectionTrafficLightSetComponent.cs to each TrafficLight . StopLine and MapLaneSegmentBuilder overlap MapLaneSegmentBuilders final waypoint needs to be slightly overlapping the MapStopLineBuilder","title":"NPC Map Navigation"},{"location":"npc-map-navigation/#npc-map-navigation","text":"Non-player character (NPC) vehicles now use the MapSegmentBuilder classes to navigate annotated maps.","title":"NPC Map Navigation"},{"location":"npc-map-navigation/#map-manager","text":"Use SanFrancisco.scene as a template to build map data for NPCs. Remove SFTraffic_New.prefab from scene and any associated scripts, e.g., TrafPerformanceManager.cs, TrafInfoManager.cs and TrafSystem.cs. Create a new GameObject at position Vector3.zero and Quaternion.identity, named Map . Add MapManager.cs component to this object and save as a prefab in project assets. The public field, SpawnLanesHolder, of MapManager.cs requires the MapLaneSegmentBuilder holder transform. The public field, IntersectionsHolder, of MapManager.cs requires the TrafficLights holder transform. This has intersection meshes and scripts for lights. The public fields Green, Yellow, and Red are materials for the segmentation camera system. If using the Traffic meshes from SanFrancisco.scene, these need to be added here. If not, IntersectionComponent.cs and associated scripts will need to be edited.","title":"Map Manager"},{"location":"npc-map-navigation/#map-lane-and-intersection-grouping","text":"Create TrafficLanes holder object as a child of Map.prefab . Place all MapLaneSegmentBuilder objects into TrafficLanes holder object for all non intersection lanes. Create IntersectionLanes holder object as a child of Map.prefab . Create a new Intersection holder object as a child of IntersectionLanes transform for each intersection annotation. Be sure its world position is in the center of each intersection. Place MapLaneSegmentBuilder and MapStopLineSegmentBuilder objects into Intersection holder object for each intersection.","title":"Map Lane and Intersection Grouping"},{"location":"npc-map-navigation/#map-intersection-builder","text":"For each Intersection holder, add the MapIntersectionBuilder.cs component.","title":"Map Intersection Builder"},{"location":"npc-map-navigation/#traffic-lights","text":"Create a TrafficLights holder object to hold all traffic light meshes or place all traffic meshes under the map annotation Intersections . Just be sure to have the root holder be in MapManager.cs IntersectionHolder public reference. Create a Intersection holder object. Be sure its world position is in the center of each intersection. Add IntersectionComponent.cs to each Intersection holder object. Place TrafficLightPole facing it's corresponding StopLineSegmentBuilder object. The transfom needs to be Z axis or gizmo arrow forward, parallel to the StopLineSegmentBuilder object Z axis or gizmo arrow forward. Add IntersectionTrafficLightSetComponent.cs. Place as a child of the Intersection holder object. For opposite facing TrafficLightPoles and StopLineSegmentBuilders , be sure to orient transforms in Z axis or gizmo arrow forward but perpendicular to other facing light poles and stoplines. Add TrafficLight meshes as children of the TrafficLightPole . Add IntersectionTrafficLightSetComponent.cs to each TrafficLight .","title":"Traffic Lights"},{"location":"npc-map-navigation/#stopline-and-maplanesegmentbuilder-overlap","text":"MapLaneSegmentBuilders final waypoint needs to be slightly overlapping the MapStopLineBuilder","title":"StopLine and MapLaneSegmentBuilder overlap"},{"location":"perception-ground-truth/","text":"Ground Truth Obstacles Overview You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles. View ground truth obstacles in Simulator Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view. Bounding box colors Green: Vehicles Yellow: Pedestrians Purple: Unknown Subscribe to ground truth ROS messages LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge. Install the lgsvl_msgs ROS package Use LGSVL Apollo or Autoware repository If you are using LGSVL's forks of Apollo or Autoware , the package is already included as a submodule in the respective workspace: LGSVL Autoware: autoware - ros - src - msgs - lgsvl_msgs LGSVL Apollo: apollo - ros_pkgs - src - lgsvl_msgs Following the instructions to build the ROS workspace will build the lgsvl_msgs package as well. If you are not running LGSVL's Apollo or Autoware forks, you can directly clone our lgsvl_msgs package into your ROS workspace and build. Manually install lgsvl_msgs Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make Subscribe to ground truth messages from Simulator You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link View estimated detections in Simulator If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"Ground truth obstacles"},{"location":"perception-ground-truth/#ground-truth-obstacles","text":"","title":"Ground Truth Obstacles"},{"location":"perception-ground-truth/#overview","text":"You can use the LGSVL Simulator to view, subscribe to, and compare ground truth obstacle information. The simulator allows visualization of 2D or 3D bounding boxes of vehicles, pedestrians, and unknown objects, and publishes detailed information (currently in a custom ROS message format) about the ground truth obstacles.","title":"Overview"},{"location":"perception-ground-truth/#view-ground-truth-obstacles-in-simulator","text":"Ground truth obstacles for traffic can be viewed in the simulator with both 3D bounding boxes as well as 2D bounding boxes in the camera. To view 3D Bounding boxes in the simulator: Start the simulator in the San Francisco map and desired vehicle Check \"Sensor Effects\" Check \"Enable Traffic\" Check \"Enable Ground Truth 3D\" You should see 3D boxes in green highlighting NPC vehicles in the simulator main view. To view 2D bounding boxes: Start the simulator in the San Francisco map with desired vehicle Check \"Ground Truth 2D\" You should see 2D boxes highlighting NPC vehicles in the \"Ground Truth 2D Camera\" camera view.","title":"View ground truth obstacles in Simulator"},{"location":"perception-ground-truth/#bounding-box-colors","text":"Green: Vehicles Yellow: Pedestrians Purple: Unknown","title":"Bounding box colors"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-ros-messages","text":"LGSVL Simulator also publishes custom ROS messages describing the ground truth data of non-ego vehicles. In order to subscribe to the ground truth messages, you will need the ROS package lgsvl_msgs . It contains custom ROS message types for 2D and 3D bounding boxes. You will also need to be running rosbridge.","title":"Subscribe to ground truth ROS messages"},{"location":"perception-ground-truth/#install-the-lgsvl_msgs-ros-package","text":"","title":"Install the lgsvl_msgs ROS package"},{"location":"perception-ground-truth/#use-lgsvl-apollo-or-autoware-repository","text":"If you are using LGSVL's forks of Apollo or Autoware , the package is already included as a submodule in the respective workspace: LGSVL Autoware: autoware - ros - src - msgs - lgsvl_msgs LGSVL Apollo: apollo - ros_pkgs - src - lgsvl_msgs Following the instructions to build the ROS workspace will build the lgsvl_msgs package as well. If you are not running LGSVL's Apollo or Autoware forks, you can directly clone our lgsvl_msgs package into your ROS workspace and build.","title":"Use LGSVL Apollo or Autoware repository"},{"location":"perception-ground-truth/#manually-install-lgsvl_msgs","text":"Clone lgsvl_msgs to your ROS workspace or msgs directory: $ git clone https://github.com/lgsvl/lgsvl_msgs {MY_ROS_WS} Build the ROS workspace: $ catkin_make","title":"Manually install lgsvl_msgs"},{"location":"perception-ground-truth/#subscribe-to-ground-truth-messages-from-simulator","text":"You can subscribe to ground truth messages published as ROS messages (when 2D/3D ground truth are enabled) Topic: /simulator/ground_truth/2d_detections Message type: lgsvl_msgs/Detection2DArray: Link Topic: /simulator/ground_truth/3d_detections Message type: lgsvl_msgs/Detection3DArray: Link","title":"Subscribe to ground truth messages from Simulator"},{"location":"perception-ground-truth/#view-estimated-detections-in-simulator","text":"If you are running Autoware with LGSVL Simulator, you can also visualize Autoware object detection outputs in the simulator for both Lidar-based and Camera-based detections. Make sure that Autoware perception module is running and detection output topics have output messages. (You can also publish to the below topics even if you are not using Autoware) Required ROS topics: - For Lidar detections: /detection/lidar_objects - For Camera detections: /detection/vision_objects To view Lidar detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Sensor Effects\" Check \"Enable LIDAR\" Check \"Enable Lidar Prediction\" You should see 3D bounding boxes highlighting Autoware Lidar detections in the simulator main view. To view Camera detections: Start the simulator in the San Francisco map with XE_Rigged-autoware vehicle Check \"Toggle Main Camera\" Check \"Enable Camera Prediction\" You should see 2D bounding boxes highlighting Autoware Camera detections in the simulator main camera view.","title":"View estimated detections in Simulator"},{"location":"python-api/","text":"Python API Overview LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retreive sensor configuration and data, control weather, time state, and more. Requirements Using Python API requires Python version 3.5 or later. Quickstart After unpacking LGSVL simulator zip file or cloning source from the git repository you should see an Api folder in the root. The Python API interface is fully contained in this folder. Go inside this folder and run the follwing command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor) and leave it running in the Menu.unity scene. Simulator by default listens for connections on port 8181. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the SanFrancisco.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move Core concepts The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values. Simulation To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator( localhost , 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load( SanFrancisco ) Map name is a string representing the name of the scene file in Unity. Currently available scenes: SanFrancisco - large city map SimpleMap - small city map SimpleRoom - for Tugbot robot SimpleLoop - for Duckiebot robot Duckietown - for Duckiebot robot DuckieDowntown - for Duckiebot robot Check the Unity project for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. The only way to advance time in the simulator is to call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution: Agents You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) This will create an EGO vehicle from the XE_Rigged-apollo template. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: XE_Rigged-apollo - Apollo 3.0 vehicle XE_Rigged-apollo_3_5 - Apollo 3.5 vehicle XE_Rigged-autoware - Autoware vehicle Tugbot - Tugbot warehouse robot duckiebot-duckietown-ros1 - Duckiebot robot for ROS1 duckiebot-duckietown-ros2 - Duckiebot robot for ROS2 Available NPC vehicles: Sedan SUV Jeep HatchBack SchoolBus DeliveryTruck Available pedestrian types: Bob Entrepreneur Howard Johnny Pamela Presley Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information. EGO vehicle EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True) NPC vehicles You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent( Sedan , lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates and a desired velocity in m/s. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent( Sedan , lgsvl.AgentType.NPC) def on_waypoint(agent, index): print( waypoint {} reached .format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn. Pedestrians You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent( Bob , lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates and an idle time that the pedestrian will spend standing in-place when it reaches the waypoint. You can receive information on progress by setting the on_waypoint_reached callback. Callbacks The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below. Agent callbacks collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = STATIC OBSTACLE if agent1 is None else agent1.name name2 = STATIC OBSTACLE if agent2 is None else agent2.name print( {} collided with {} at {} .format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point. NpcVehicle callbacks In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance Pedestrian callbacks In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer. Sensors EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge Camera Sensor The Camera sensor has the following read only properties: frequency - rate at which images are captured sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = Main Camera : sensor.save( main-camera.png , compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files. Lidar Sensor Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = velodyne : sensor.save( lidar.pcd ) A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255). IMU Sensor You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent. GPS Sensor You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print( Latitude: , data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees Radar Sensor Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor. CAN bus Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor. Weather and Time of Day Control You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print( Current time of day: , sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ). Helper Functions Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print( Distance right: , hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values. Changelog 2019-04-19 initial release","title":"Python API"},{"location":"python-api/#python-api","text":"","title":"Python API"},{"location":"python-api/#overview","text":"LGSVL Simulator exposes runtime functionality to a Python API which you can use to manipulate object placement and vehicle movement in a loaded scene, retreive sensor configuration and data, control weather, time state, and more.","title":"Overview"},{"location":"python-api/#requirements","text":"Using Python API requires Python version 3.5 or later.","title":"Requirements"},{"location":"python-api/#quickstart","text":"After unpacking LGSVL simulator zip file or cloning source from the git repository you should see an Api folder in the root. The Python API interface is fully contained in this folder. Go inside this folder and run the follwing command to install Python files and necessary dependencies: pip3 install --user -e . Now launch the simulator (either binary .exe file or from Unity Editor) and leave it running in the Menu.unity scene. Simulator by default listens for connections on port 8181. Run the following example to see the API in action: ./quickstart/05-ego-drive-in-circle.py This will load the SanFrancisco.unity scene, instantiate one EGO vehicle, then ask you to press Enter to start driving EGO vehicle in a circle. When the script is running, it will apply throttle and steering commands to make the car move","title":"Quickstart"},{"location":"python-api/#core-concepts","text":"The Simulator and API communicate by sending json over a websocket server running on 8181 port. The API client can be either on the same machine or any other machine on the same network. API exposes the following main types: Simulator - main object for connecting to simulator and creating other objects Agent - superclass of vehicles and pedestrian EgoVehicle - EGO vehicle with accurate physics simulation and sensors NpcVehicle - NPC vehicle with simplified physics, useful for creating many background vehicles Pedestrian - pedestrian walking on sidewalks Vehicles and Pedestrian are a subclasses of Agent which has common properties like transform, position, and velocity. All coordinates in the API return values in the Unity coordinate system. This coordinate system uses meters as a unit of distance and is a left-handed coordinate system - x points left, z points forward, and y points up. The Simulator class provides helper methods to convert coordinates to and from latitude/longitude and northing/easting values.","title":"Core concepts"},{"location":"python-api/#simulation","text":"To connect to the simulator you need to an instance of the Simulator class: import lgsvl sim = lgsvl.Simulator( localhost , 8181) You can specify a different address as hostname or IP address. By default only port 8181 is used for API connection. Only one client can be connected to simulator at a time. Next, load the scene (\"map\"). This is done by load method: sim.load( SanFrancisco ) Map name is a string representing the name of the scene file in Unity. Currently available scenes: SanFrancisco - large city map SimpleMap - small city map SimpleRoom - for Tugbot robot SimpleLoop - for Duckiebot robot Duckietown - for Duckiebot robot DuckieDowntown - for Duckiebot robot Check the Unity project for a full list of available scenes. Once a scene is loaded you can instantiate agents and run simulations. See the Agents section on how to create vehicles and pedestrians. Loading scenes takes a while, to reset a scene to the initial state without reloading it call the reset method: sim.reset() This will remove any vehicles or callbacks currently registered. After setting up the scene in a desired state you can start advancing time. During python code execution time is stopped in the simulator. The only way to advance time in the simulator is to call the run method: sim.run(time_limit = 5.0) run accepts an optional argument for a time limit specifying how long to run. The default value of 0 will run infinitely. Diagram illustrating API execution:","title":"Simulation"},{"location":"python-api/#agents","text":"You can create vehicles and pedestrians by calling the add_agent method of the Simulator object. Example: ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) This will create an EGO vehicle from the XE_Rigged-apollo template. Other AgentTypes available are: AgentType.EGO - EGO vehicle AgentType.NPC - NPC vehicle AgentType.PEDESTRIAN - pedestrian Each agent type has predefined names you can use. Currently availble EGO vehicles: XE_Rigged-apollo - Apollo 3.0 vehicle XE_Rigged-apollo_3_5 - Apollo 3.5 vehicle XE_Rigged-autoware - Autoware vehicle Tugbot - Tugbot warehouse robot duckiebot-duckietown-ros1 - Duckiebot robot for ROS1 duckiebot-duckietown-ros2 - Duckiebot robot for ROS2 Available NPC vehicles: Sedan SUV Jeep HatchBack SchoolBus DeliveryTruck Available pedestrian types: Bob Entrepreneur Howard Johnny Pamela Presley Robin Stephen Zoe If an incorrect name is entered, a Python exception will be thrown. Optionally you can create agents in specific positions and orientations in the scene. For this you need to use the AgentState class. For example: state = lgsvl.AgentState() state.transform.position = lgsvl.Vector(10, 0, 30) state.transform.rotation.y = 90 ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO, state) This will create a vehicle at position x=10, z=30 which is rotated 90 degrees around the vertical axis. The position and rotation are set in the world coordinates space. You can always adjust the position, rotation, velocity and angular velocity of the agent at any later time: s = ego.state s.velocity.x = -50 ego.state = s This will set x component of velocity (in world coordinate space) to -50 meters per second and leave y and z components of velocity unmodified. All agents have the following common functionality: state - property to get or set agent state (position, velocity, ...) transform - property to get transform member of the state (shortcut for state.transform ) bounding_box - property to get bounding box in local coordinate space. Note that bounding box is not centered around (0,0,0) - it depends on the actual geometry of the agent. on_collision - method to set a callback function to be called when the agent collides with something (other agent or static obstacle), see callbacks section for more information.","title":"Agents"},{"location":"python-api/#ego-vehicle","text":"EGO vehicle has following additional functionality: apply_control - method to apply specified throttle, break, steering or other actions to vehicle. Pass sticky=True to apply these values on every simulation update iteration. get_sensors - method to return list of sensors connect_bridge - method to connect to ROS or Cyber RT bridge bridge_connected - bool property, True if bridge is connected You can control the movement of the EGO vehicle either by manually specifying state, applying manual control, or connecting through the bridge. Example to apply constant 20% throttle to EGO vehicle: ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) c = lgsvl.VehicleControl() c.throttle = 0.2 ego.apply_control(c, True)","title":"EGO vehicle"},{"location":"python-api/#npc-vehicles","text":"You can create multiple NPC vehicles on the map to drive along the lanes or follow specific waypoints on the map. NPC vehicle has the following additional functionality: change_lane - method to make the vehicle change lanes follow - method to make vehicle follow specific waypoints follow_closest_lane - method to make vehicle follow lanes on_waypoint_reached - method to set callback function which is called for every waypoint the vehicle reaches on_stop_line - method to set callback function which is called when vehicle reaches a stop line at interesection on_lane_change - method to set callback function which is called when vehicle decides to change lanes You can control the movement of an NPC vehicle either by manually specifying state, or instructing it to follow waypoints or lanes. To make an NPC follow waypoints prepare a list of DriveWaypoint objects and call the follow method for the npc vehicle: npc = sim.add_agent( Sedan , lgsvl.AgentType.NPC) waypoints = [ lgsvl.DriveWaypoint(lgsvl.Vector(1,0,3), 5), lgsvl.DriveWaypoint(lgsvl.Vector(5,0,3), 10), lgsvl.DriveWaypoint(lgsvl.Vector(1,0,5), 5), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates and a desired velocity in m/s. The NPC will ignore all traffic rules and will not avoid collisions to try to get to the next waypoint. You can receive information on progress by setting the on_waypoint_reached callback. Example (see callbacks for more details): npc = sim.add_agent( Sedan , lgsvl.AgentType.NPC) def on_waypoint(agent, index): print( waypoint {} reached .format(index)) npc.follow(waypoints, loop=True) npc.on_waypoint_reached(on_waypoint) sim.run() follow_closest_lane will make the NPC vehicle follow whatever lane is the closest. Upon reaching intersections it will randomly decide to either drive straight or turn.","title":"NPC vehicles"},{"location":"python-api/#pedestrians","text":"You can create Pedestrian agents that will allow you to create pedestrians on sidewalks and make them walk. Pedestrians have the following additional functionality: walk_randomly - method to make pedestrian walk randomly on the sidewalk follow - method to make pedestrian follow specific waypoints on_waypoint_reached - method to set callback function which is called for every waypoint reached You can control the movement of pedestrians either by manually specifying state, or instructing them to follow waypoints or walk randomly. To make pedestrians follow waypoints prepare a list of WalkWaypoint objects and call the follow method for pedestrians: npc = sim.add_agent( Bob , lgsvl.AgentType.PEDESTRIAN) waypoints = [ lgsvl.WalkWaypoint(lgsvl.Vector(1,0,3), 5), lgsvl.WalkWaypoint(lgsvl.Vector(5,0,3), 10), lgsvl.WalkWaypoint(lgsvl.Vector(1,0,5), 5), ] npc.follow(waypoints, loop=True) Each waypoint has a position in world coordinates and an idle time that the pedestrian will spend standing in-place when it reaches the waypoint. You can receive information on progress by setting the on_waypoint_reached callback.","title":"Pedestrians"},{"location":"python-api/#callbacks","text":"The Python API can invoke callbacks to inform you of specific events that occur during simulator runtime. Callbacks are invoked from inside the Simulator.run method and while a callback is running the simulation time is paused. Once the callback finishes time is resumed and the simulation resumes execution. You can call Simulator.stop to stop further execution and return immediately from the callback. The internals of this process are illustrated in the following sequence diagram: Here the code resumes simulation after the first callback, but stops execution when the second callback is handled. You set callback functions by calling on_NAME method of object, see information below.","title":"Callbacks"},{"location":"python-api/#agent-callbacks","text":"collision - called when agent collides with something (other agent or stationary obstacle). Example usage: def on_collision(agent1, agent2, contact): name1 = STATIC OBSTACLE if agent1 is None else agent1.name name2 = STATIC OBSTACLE if agent2 is None else agent2.name print( {} collided with {} at {} .format(name1, name2, contact)) ego.on_collision(on_collision) Callback receives three arguments: (agent1, agent2, contact) - the first two are the agents that collide, one of them can be None if it is a stationary obstacle like a building or a traffic light pole, and the third is the world position of the contact point.","title":"Agent callbacks"},{"location":"python-api/#npcvehicle-callbacks","text":"In addition to Agent callbacks, NpcVehicle has three extra callbacks: waypoint_reached - called when vehicle reaches a waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer stop_line - called when vehicle stops at a stop line for a traffic light or stop sign; accepts one argument: (agent) - agent instance lane_change - called when vehicle starts changing lane; accepts one argument: (agent) - agent instance","title":"NpcVehicle callbacks"},{"location":"python-api/#pedestrian-callbacks","text":"In addition to Agent callbacks, Pedestrian has one extra callback. waypoint_reached - called when pedestrian reaches waypoint; accepts two arguments: (agent, index) - agent instance and waypoint index as integer.","title":"Pedestrian callbacks"},{"location":"python-api/#sensors","text":"EGO vehicles have sensors attached. You can get a list of them by calling EgoVehicle.get_sensors() which returns a Python list with instances of the following classes: CameraSensor - see Camera sensor LidarSensor - see Lidar sensor ImuSensor - see IMU sensor GpsSensor - see GPS sensor RadarSensor - see Radar sensor CanBusSensor - see CAN bus Each sensor has the following common members: name - name of sensor, to diffrentiate sensors of the same type, for example, to choose one out of multiple cameras attached to EgoVehicle transform - property that contains position and rotation of a sensor relative to the agent transform enabled - bool property, set to True if sensor is enabled for capturing and sending data to ROS or Cyber bridge","title":"Sensors"},{"location":"python-api/#camera-sensor","text":"The Camera sensor has the following read only properties: frequency - rate at which images are captured sent to ROS or Cyber bridge width - image width height - image height fov - vertical field of view in degrees near_plane - distance of near plane far_plane - distance of far plane format - format of image (\"RGB\" for 24-bit color image, \"DEPTH\" - 8-bit grayscale depth buffer, \"SEMANTIC\" - 24-bit color image with sematic segmentation) Camera image can be saved to disk by calling save : ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = Main Camera : sensor.save( main-camera.png , compression=0) save method accepts a path relative to the running simulator, and an optional compression for png files (0...9) or quality (0..100) for jpeg files.","title":"Camera Sensor"},{"location":"python-api/#lidar-sensor","text":"Lidar sensor has following read only properties: min_distance - minimal distance for capturing points max_distance - maximum distance for capturing points rays - how many laser rays (vertically) to use rotations - frequency of rotation, typically 10Hz measurements - how many measurmenets per rotation each ray is taking fov - vertical field of view (bottom to top ray) in degrees angle - angle lidar is tilted (middle of fov view) compensated - bool, whether lidar point cloud is compensated Lidar point cloud can be saved to disk as a .pcd file by calling save : ego = sim.add_agent( XE_Rigged-apollo , lgsvl.AgentType.EGO) for sensor in ego.get_sensors(): if sensor.name = velodyne : sensor.save( lidar.pcd ) A .pcd file is in the binary Point Cloud Data format where each point has x/y/z coordinates as 4-byte floats and a 1-byte unsigned int as intensity (0...255).","title":"Lidar Sensor"},{"location":"python-api/#imu-sensor","text":"You can use the IMU sensor to get its position in the vehicle. All measurements an IMU would provide can be obtained by using the transform property of the agent.","title":"IMU Sensor"},{"location":"python-api/#gps-sensor","text":"You can retrieve the current GPS location from the GPS sensor by calling data : data = gps_sensor.data() print( Latitude: , data.latitude) Returned data will contain following fields: latitude longitude northing easting altitude orientation - rotation around up-axis in degrees","title":"GPS Sensor"},{"location":"python-api/#radar-sensor","text":"Currently the Radar sensor can be used only to get its position and rotation in the vehicle. Radar measurements can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"Radar Sensor"},{"location":"python-api/#can-bus","text":"Currently CAN bus can be used only to get its position and rotation in the vehicle. CAN bus messages can be received in ROS or Cyber by setting the enabled property of the sensor.","title":"CAN bus"},{"location":"python-api/#weather-and-time-of-day-control","text":"You can control the weather properties of the simulation by reading or writing to the weather property. You can set rain , fog or wetness (float 0...1). Example: w = sim.weather w.rain = 0.5 # set rain to 50% sim.weather = w Changing time of day allows to control whether the loaded scene appears as day or night. To get the current time read the time_of_day property: print( Current time of day: , sim.time_of_day) It will return a float between 0 and 24. To set time of day call set_time_of_day : sim.set_time_of_day(10, fixed=True) This will set current time of day to 10am. The optional bool argument fixed indicates whether the simulation should advance this time automatically or freeze it and not change it ( fixed=True ).","title":"Weather and Time of Day Control"},{"location":"python-api/#helper-functions","text":"Simulator class offers following helper functions: version - property that returns current version of simulator as string current_scene - property that returns currently loaded scene as string, None if none is loaded current_frame - property that returns currently simulated frame number as integer current_time - property that returns currentl simulation time in seconds as float get_spawn - method that returns list of transforms representing good positions where to place vehicles in the map. This list can be empty, it depends on how the map is prepared in Unity. Returned transforms contain position and rotation members as a Vector get_agents - method that returns a list of currently available agent objets added with add_agent To map points in Unity coordinates to GPS coordinates the Simulator class offers the following two functions: map_to_gps - maps transform (position rotation) to GPS location, returns same type as GPS Sensor data method map_from_gps - maps GPS location (latitude/longitude or northing/easting) to transform raycast - shoots a ray from specific location and returns closest object it hits map_from_gps accepts two different inputs - latitude/longitude or northing/easting. Examples: tr1 = sim.map_from_gps(latitude=10, longitude=-30) tr2 = sim.map_from_gps(northing=123455, easting=552341) Optionally you can pass altitude and orientation. raycast method can be used in following way: origin = lgsvl.Vector(10, 0, 20) direction = lgsvl.Vector(1, 0, 0) hit = sim.raycast(origin, direction, layer_mask=1) if hit: print( Distance right: , hit.distance) This will shoot a ray in the positive x-axis direction from the (10,0,20) coordinates. A RaycastHit object with distance , point and normal fields is returned if something is hit, otherwise None is returned. When raycasting you should specify a layer_mask argument that specifies which objects to check collision with. It corressponds to layers in the Unity project - check the project for actual values.","title":"Helper Functions"},{"location":"python-api/#changelog","text":"2019-04-19 initial release","title":"Changelog"}]}